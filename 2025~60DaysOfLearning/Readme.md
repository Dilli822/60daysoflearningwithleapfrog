# 60 Days of Learning: LearningWithLeapfrog2025

Welcome to my #60DaysOfLearning journey!
I‚Äôm learning AI, Machine & Deep learning ‚Äîone day at a time.

üìÖ **Start Date: June 1 , 2025**
üê¶ **Follow my journey on Twitter:**[ @dilli_hangrae](https://x.com/dilli_hangrae)

---

## üìä Progress Tracker

| Day   | Topic Covered                                                                                                                                       | Tweet Link                                                                 | Mathematics Notes                                                                                 |
|-------|-----------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|
| Day 1 | Intro to PyTorch Tensors & Autograd                                                                                                                | [Tweet](https://x.com/dilli_hangrae/status/1929161284739629406)             | Learned about `torch.Tensor`, `.requires_grad`, gradient tracking, etc.                           |
| Day 2 | Building Neural Nets with `nn.Module`                                                                                                               | [Tweet](https://x.com/dilli_hangrae/status/1929548932117406131)             | Forward pass, custom layers, loss, optimizer training explained                                   |
| Day 3 | PyTorch Multi-Class Classification, Non-linearity                                                                                                  | [Tweet](https://x.com/dilli_hangrae/status/1929926749447442759)             | L2 Regression, Softmax, PyTorch multi-class setup                                                 |
| Day 4 | Feature Squeezing, PyTorch for Computer Vision                                                                                                     | [Tweet](https://x.com/dilli_hangrae/status/1930299131677028402)             | Vector flattening, dimensionality handling, L2 derivation with partial derivatives                |
| Day 5 | Model Training Loop: Train + Test + Predict + Eval                                                                                                  | [Tweet](https://x.com/dilli_hangrae/status/1930666286301819026)             | L1 (Lasso), Elastic Net regression and cost function math                                         |
| Day 6 | Pre-trained CV Models & Transfer Learning                                                                                                           | [Tweet](https://x.com/dilli_hangrae/status/1930994441810002315)             | Multinomial Logistic Regression math and interpretation                                            |
| Day 7 | Adjusted R¬≤, MSE, MAE, RSS Metrics + Fine-tuned CV model (Val Acc: 91%)                                                                             | [Tweet](https://x.com/dilli_hangrae/status/1931371588630221251)             | Adjusted vs. Regular R¬≤, detailed metrics for regression                                          |
| Day 8 | Modular Code, Impurity Metrics (Entropy, Gini, Info Gain), Decision Trees + R¬≤ Visuals                                                              | [Tweet](https://x.com/dilli_hangrae/status/1932080670848991547)             | Information Gain vs. Gain Ratio, impurity-based decision making                                   |
| Day 9 | Early Stopping Theory, Decision Tree Inducers, Attribute Splits, Experiment Tracking in PyTorch                                                    | [Tweet](https://x.com/dilli_hangrae/status/1932443038833672382)             | Attribute selection metrics: Gini, Entropy, Split Info                                            |
| Day10| Ergodicity Assumptions, Pseudo Inverse, Linear Least Squares Filter with Full Derivations                                                           | [Tweet](https://x.com/dilli_hangrae/status/1932863310262882574)             | Mathematics behind filters, least squares optimization, pseudo-inverse concepts                   |
| Day11 |  Recurrent Neural Networks Over Naive Neural Networks,Assumptions and Convergence in LMS, PyTorch LMS & RNN Basics| [Tweet] https://x.com/dilli_hangrae/status/1933249849626898817 | 
| Day12 |  Pruning Types, Cost Complexity Pruning Basic Ideas, Overfitting and its causes in DT, Occam's Razor Principle, Early Stopping Basics | [Tweet] https://x.com/dilli_hangrae/status/1933907244610949143 | 
| Day13 | Theories on KD Trees, Native Brute Force, Distance Methods, Detailed SK Labs on Beyond Linear Models:Classification Focus, KNN, DT, SVM, Naive Bayes | [Tweet] https://x.com/dilli_hangrae/status/1933249849626898817 | 
| Day14 | Theories on Ball Trees,SVM,RBF, Kernel, Kernel Trick, Maximum Margin Classifier, Nearest Neigbhour, Labs on beyond linear models | [Tweet] https://x.com/dilli_hangrae/status/1933249849626898817 
| Day15 | Distance Metrics: Euclidean, Manhattan, & Mahalanobis, brute force NN, Minkowski Distance theories and mathematics, Lab on DT Implementation SK Learn | [Tweet] https://x.com/dilli_hangrae/status/1935004783930421752
| Day16 | Error Decomposition For Classification & Regression, Image Preprocessing using inbuilt opencv functions opening,closing, Model training History Plotting | [Tweet] https://x.com/dilli_hangrae/status/1935369839935431127
| Day17 | Theories on Bootstrapping & Bagging with Pros and Detailed Boosting, Labs on Error Decomposition and Examples on Boostring and Bagging with some plots | [Tweet] https://x.com/dilli_hangrae/status/1935731677751603245


