{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQTAV-3G4k7u"
      },
      "source": [
        "# Computer Vision in PyTorch\n",
        "\n",
        "Computer vision is the art of teaching a computer to see.\n",
        "\n",
        "For example, it could involve building a model to classify whether a photo is of a cat or a dog (binary classification).\n",
        "\n",
        "Or whether a photo is of a cat, dog or chicken (multi-class classification).\n",
        "\n",
        "Or identifying where a car appears in a video frame (object detection).\n",
        "\n",
        "Or figuring out where different objects in an image can be separated (panoptic segmentation).\n",
        "\n",
        "### What we cover?\n",
        "#### 0. Computer Vision Libraries in PyTorch\n",
        "#### 1. Load Data\n",
        "#### 2. Prepare Data\n",
        "\n",
        "\n",
        "##### The torch.utils.data.Dataset and torch.utils.data.DataLoader classes aren't only for computer vision in PyTorch, they are capable of dealing with many different types of data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N99DQvV3-Qxa",
        "outputId": "eb81ccfc-6770-4589-9b89-d181fadb50d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Version:  2.6.0+cu124 | torch version: 0.21.0+cu124\n"
          ]
        }
      ],
      "source": [
        "# Import torch vision\n",
        "import torch\n",
        "import math\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from helper_functions import accuracy_fn # importing the accuracy function\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(f\"PyTorch Version:  {torch.__version__} | torch version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erEQzIC4AQcz"
      },
      "source": [
        "**torch — core PyTorch**\n",
        "\n",
        "**torchvision — main vision package**\n",
        "\n",
        " **datasets — built-in datasets**\n",
        "\n",
        " **models — pretrained model architectures**\n",
        "\n",
        " **transforms — image preprocessing**\n",
        "\n",
        " **Dataset — base class for datasets**\n",
        "\n",
        " **DataLoader — batching and loading databold**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkMn12d4urOa"
      },
      "outputs": [],
      "source": [
        "# Setting the see and start the timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-MltSxVApT8",
        "outputId": "3c69351b-ba8c-4400-f6fb-7bafaee02d55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. GETTING A DATASET  EMNIST\n",
            "transformed [ToTensor()]\n",
            "=== LETS UNDERSTAND THE BUCN OF COMPUTER VISION DATASETS STORED IN TORCHVISION.DATASETS ===\n",
            "1. root: str mean which folder to download the load to\n",
            "2. train: bool want to train or test split?\n",
            "3. download: should the data to be downloaded?\n",
            "4. Apply transformation using torchvision.transforms and target_transform to transform the targets (lables)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 562M/562M [09:56<00:00, 942kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EMNIST dataset downloaded and loaded!!\n",
            "Number of training sample: 112800\n",
            "Number of test samples: 18800\n"
          ]
        }
      ],
      "source": [
        "print(\"1. GETTING A DATASET  EMNIST\")\n",
        "\n",
        "# Transform | can add resize,normalize, flips and other data augmentation techniques here\n",
        "transform = transforms.Compose([transforms.ToTensor()]) # since transform is just a container\n",
        "print(\"transformed\", transform.transforms)\n",
        "\n",
        "\n",
        "print(\"=== LETS UNDERSTAND THE BUCN OF COMPUTER VISION DATASETS STORED IN TORCHVISION.DATASETS ===\")\n",
        "print(\"1. root: str mean which folder to download the load to\")\n",
        "print(\"2. train: bool want to train or test split?\")\n",
        "print(\"3. download: should the data to be downloaded?\")\n",
        "print(\"4. Apply transformation using torchvision.transforms and target_transform to transform the targets (lables)\")\n",
        "\n",
        "\n",
        "# download the emnist balanced split(digits+letters) online\n",
        "train_dataset = datasets.EMNIST(root=\"./data\", split=\"balanced\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.EMNIST(root=\"./data\", split=\"balanced\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_data = train_dataset\n",
        "test_data = test_dataset\n",
        "\n",
        "print(f\"EMNIST dataset downloaded and loaded!!\")\n",
        "print(f\"Number of training sample: {len(train_dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LMT1N2qSDv_1",
        "outputId": "92db2ca2-4fe1-4fa0-b510-50f04622abe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image 0 shape: torch.Size([1, 28, 28]) | label: 45\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAADECAYAAAAGYxrSAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAC71JREFUeJzt3W1M1eUbB/DvD0I5nTYaT2XNJ+bjCVvOfAipfGKW5oabbTrNUVsvmq7WorIXis4XarPJGqZsDhGdvqgB5Vo5S3SmjKO23LDsSZHZUPEkghFHgfv/on9n2n396HfCg1yH72fjhRfXOeeWfbnHff+eHGOMAZEyCfd6AET/BYNLKjG4pBKDSyoxuKQSg0sqMbikEoNLKjG4pBKDK2hoaIDjONi8efNde8/Dhw/DcRwcPnz4rr3nQBY3wS0vL4fjODh58uS9HkqfyMvLg+M4WLlypfU9x3HEr40bN96DkcbGffd6ABS9yspK1NbW9tiTl5eH5cuX31GbOHFiLIfVpxhcZTo6OvDWW2/h3XffxZo1a1z7xowZg2XLlvXhyPpW3Pyp4MXNmzexZs0aTJo0CSkpKfD7/Xj66adRU1Pj+potW7Zg+PDh8Pl8ePbZZ1FfX2/1nD17FosWLUJqaiqSk5Px5JNP4rPPPvvX8bS3t+Ps2bO4evWq5//D+++/j+7ubhQWFv5r759//omOjg7P763JgApua2srduzYgRkzZmDTpk1Yu3YtmpubMXfuXHz33XdWf0VFBT788EOsWLEC7733Hurr6zFr1ixcvnw50nPmzBlMmzYNP/zwA1atWoUPPvgAfr8f+fn5qKqq6nE8wWAQ48ePR0lJiafxNzY2YuPGjdi0aRN8Pl+PveXl5fD7/fD5fAgEAti7d6+nz1DDxImdO3caAObEiROuPZ2dnSYcDt9Ru3btmnnooYfMK6+8EqmdP3/eADA+n89cvHgxUq+rqzMAzJtvvhmpzZ4920yYMMF0dHREat3d3SYnJ8eMHj06UqupqTEATE1NjVUrKiry9H9ctGiRycnJifwbgFmxYoXVl5OTY4qLi82nn35qtm3bZrKzsw0A89FHH3n6HA0GVHBv19XVZUKhkGlubjbz5883TzzxROR7fwd3yZIl1uumTp1qxo4da4wxJhQKGcdxzPr1601zc/MdX+vWrTMAIsGXghuNQ4cOGcdxTDAYjNTcgvtP4XDYZGdnmwcffNC0t7f/p8/vbwbUnwoAsGvXLjz++ONITk5GWloaMjIy8Pnnn+P69etW7+jRo63amDFj0NDQAAD45ZdfYIzB6tWrkZGRccdXUVERAODKlSu9HnNnZydef/11vPTSS5g8eXLUrx80aBBWrlyJlpYWnDp1qtfj6Q8G1K7Cnj17UFBQgPz8fLz99tvIzMxEYmIiNmzYgF9//TXq9+vu7gYAFBYWYu7cuWLPqFGjejVm4K+/tX/88UeUlpZGfmn+1tbWhoaGBmRmZuL+++93fY+hQ4cCAH7//fdej6c/GFDB/eSTT5CVlYXKyko4jhOp/z07/tPPP/9s1X766SeMGDECAJCVlQUASEpKwpw5c+7+gP+vsbERt27dwvTp063vVVRUoKKiAlVVVcjPz3d9j3PnzgEAMjIyYjXMPjWg/lRITEwEAJjbrg+tq6tz3cyvrq7Gb7/9Fvl3MBhEXV0dnn/+eQBAZmYmZsyYgdLSUjQ1NVmvb25u7nE8XrfDFi9ejKqqKusLAObNm4eqqipMnTrV9TPb2tpQXFyM9PR0TJo0qcfP0iLuZtyysjJ8+eWXVv2NN97ACy+8gMrKSixcuBDz58/H+fPnsX37dgQCAdy4ccN6zahRo5Cbm4vXXnsN4XAYxcXFSEtLwzvvvBPp2bp1K3JzczFhwgS8+uqryMrKwuXLl1FbW4uLFy/i9OnTrmMNBoOYOXMmioqKsHbtWte+cePGYdy4ceL3Ro4cecdMu3XrVlRXV2PBggUYNmwYmpqaUFZWhsbGRuzevRuDBg1y/RxN4i6427ZtE+sFBQUoKCjApUuXUFpaigMHDiAQCGDPnj34+OOPxZNfli9fjoSEBBQXF+PKlSuYMmUKSkpKMGTIkEhPIBDAyZMnsW7dOpSXlyMUCiEzMxMTJ07s8chWrEyfPh3Hjx/Hjh07EAqF4Pf7MWXKFJSVlWHWrFl9Pp5YcYzhfRVInwH1Ny7FDwaXVGJwSSUGl1RicEklBpdUYnBJJc8HIG4/tk8UK14PK3DGJZUYXFKJwSWVGFxSicEllRhcUonBJZUYXFKJwSWVGFxSSf01Z26HoqWLAm+/Vux2991n/xikG4QA8n0Jurq6ehoixQBnXFKJwSWVGFxSicElldQvzgYPHizWH374Yau2cOFCsdfv91u1M2fOiL3Hjh2zam4LuXA4LNap9zjjkkoMLqnE4JJKDC6pxOCSSqp2FRIS7N+zvLw8sVfaQXjxxRfFXunw8LVr18TeL774wqq5PZ/36NGjVq21tVXsDYVCVo030nTHGZdUYnBJJQaXVGJwSSXPt9LvD7dgks6bXbVqldi7ZMkSqzZ+/Phej0E6jNvW1ib2/vOZZADEZwYDwOrVq63a7c8MHih4CyaKawwuqcTgkkoMLqnE4JJKqg75Sv744w+x3t7eHpPPk05cT0pKEnuTk5M990pPWW9paRF7b968adUG2uFhzrikEoNLKjG4pBKDSyqpWpx1dnZatYMHD4q90sLo0UcfFXtTUlKsmtvVw9I5wVINAB544AGrFggExN6SkhKrtnnzZrG3rq7OqkmHlwGgu7vbU00bzrikEoNLKjG4pBKDSyoxuKSSql0FyaVLl8R6MBi0at98843Ym5WVZdVGjBgh9qampnofnCCaQ77PPfec5/eQdlwA+SR36YpibTjjkkoMLqnE4JJKDC6ppOoq32gkJiZatYyMDLE3PT3dqhUWFoq9S5cu9fRZgPwzi+a8WbcbQ0sLrvr6erFXOiS+ZcsWsbejo8Pz2GKFV/lSXGNwSSUGl1RicEklBpdUUn/INxqPPfaYWJ82bZpVe+aZZ8Retx0Er9x2Z6TVtNvJ7FJ98uTJYq90Mvu+ffvEXreT0fsjzrikEoNLKjG4pBKDSyqpX5y5LXakBYy0CAPk816HDBnSu4FFqbeHh93EwxW9Es64pBKDSyoxuKQSg0sqMbikkvpdhbS0NLH+1FNPWbWXX35Z7JXuKeZ2uDUaXV1dVs1tp0DaVXC7J5m0U3Do0CGxt7Ky0qo1NTWJvZpwxiWVGFxSicEllRhcUkn94mzYsGFiXTqf1q1Xekawm1u3blm169evi71Hjx61ahcuXBB7pbG5nT8sLc6+/vprsffIkSNWze3qYU0445JKDC6pxOCSSgwuqcTgkkqqdhWkQ6DDhw8Xe0eOHOnp9dGSdhBOnz4t9kpX07pdSTt06FCrlp2d7XlcbmNobW31/B6acMYllRhcUonBJZUYXFJJ/eLMbQEjHS6NZnHmdnWsdBjX7ZZG1dXVVk06RxcAvv32W6u2f//+HkZ4J7fx8ipfon6EwSWVGFxSicEllRhcUknVroLEbaegt4d33U62Pn78uFWTdgQA9+frSqSrf+N1R+Bu4IxLKjG4pBKDSyoxuKSS+sVZrNy4cUOsnzt3zqpJz9al2OKMSyoxuKQSg0sqMbikEoNLKnFXwUVLS4tY//777z33UuxwxiWVGFxSicEllRhcUomLsyhF8yQdih3OuKQSg0sqMbikEoNLKjG4pBKDSyoxuKQSg0sqMbikEoNLKqk/5DvQbmhMf+GMSyoxuKQSg0sqMbikEoNLKqnfVXC7x5db3SvuSvRvnHFJJQaXVGJwSSUGl1RStTiTnmJz8OBBsTc5OdmqPfLII2Kvz+ezarW1tWKvdLslLuT6HmdcUonBJZUYXFKJwSWVGFxSyTEeb3zlOE6sx/KfJCUlifWUlBSrlpubK/amp6dbta+++krsvXDhglXjvcPuHq8/S864pBKDSyoxuKQSg0sqqV+cuUlIsH8nU1NTxd7BgwdbtatXr4q94XC4dwOjHnFxRnGNwSWVGFxSicEllRhcUiludxVIJ+4qUFxjcEklBpdUYnBJJQaXVGJwSSUGl1RicEklBpdUYnBJJc/3DuOVrNSfcMYllRhcUonBJZUYXFKJwSWVGFxSicEllRhcUonBJZX+Bxox9aZfnWT4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image 1 shape: torch.Size([1, 28, 28]) | label: 36\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image 2 shape: torch.Size([1, 28, 28]) | label: 43\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image 3 shape: torch.Size([1, 28, 28]) | label: 15\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image 4 shape: torch.Size([1, 28, 28]) | label: 4\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMAGE SHAPE ---> C, H, W torch.Size([1, 28, 28])\n",
            "[color_channels=1, height=28, width=28]\n"
          ]
        }
      ],
      "source": [
        "## viewing the training sample\n",
        "image, label = train_dataset[5]\n",
        "# print(image, label)\n",
        "for i in range(5):\n",
        "  image, label = train_dataset[i]\n",
        "  print(f\"image {i} shape: {image.shape} | label: {label}\")\n",
        "\n",
        "  plt.figure(figsize=(2,2))\n",
        "\n",
        "  # image is a tensor of shape(1, 28,28) we need to sequeze to (28,28) for plotting\n",
        "  plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "  plt.title(f\"Label: {label}\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# checking the iamge shape\n",
        "print(\"IMAGE SHAPE ---> C, H, W\", image.shape)\n",
        "print(\"[color_channels=1, height=28, width=28]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9qgCjpOo8MJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CGlFaK5Ffue"
      },
      "outputs": [],
      "source": [
        "# EMNIST balanced mapping of labels (0-46) to characters:\n",
        "num_images = 10\n",
        "cols = 5\n",
        "rows = math.ceil(num_images / cols)\n",
        "\n",
        "for i in range(num_images):\n",
        "    image, label = train_dataset[i]\n",
        "    img = image.squeeze().numpy()\n",
        "    img_rgb = np.stack([img]*3, axis=2)\n",
        "\n",
        "    plt.subplot(rows, cols, i+1)\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.title(f\"Label: {label}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()  # Adjust spacing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq4tHu-KG5df"
      },
      "outputs": [],
      "source": [
        "# Load the label mapping\n",
        "# EMNIST balanced classes: 0-9 digits, A-Z uppercase, a-z lowercase (in total 47 classes)\n",
        "# The actual EMNIST balanced mapping includes digits + letters but skips some lowercase letters\n",
        "# Here's a safe approximation including digits, uppercase A-Z, and common lowercase letters present in EMNIST balanced:\n",
        "\n",
        "label_to_char = {\n",
        "    # 0: '0', 1: '1', 2: '2', 3: '3', 4: '4',\n",
        "    # 5: '5', 6: '6', 7: '7', 8: '8', 9: '9',\n",
        "    # 10: 'A', 11: 'B', 12: 'C', 13: 'D', 14: 'E', 15: 'F', 16: 'G', 17: 'H', 18: 'I', 19: 'J',\n",
        "    # 20: 'K', 21: 'L', 22: 'M', 23: 'N', 24: 'O', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'T',\n",
        "    # 30: 'U', 31: 'V', 32: 'W', 33: 'X', 34: 'Y', 35: 'Z',\n",
        "    # 36: 'a', 37: 'b', 38: 'd', 39: 'e', 40: 'f', 41: 'g', 42: 'h', 43: 'n', 44: 'q', 45: 'r', 46: 't'\n",
        "}\n",
        "\n",
        "# Now plot first 5 images with correct labels\n",
        "plt.figure(figsize=(12, 3))\n",
        "\n",
        "for i in range(5):\n",
        "    image, label = train_dataset[i]\n",
        "    img = image.squeeze().numpy()\n",
        "    img_rgb = np.stack([img]*3, axis=2)\n",
        "\n",
        "    plt.subplot(1, 5, i+1)\n",
        "    plt.imshow(img_rgb)\n",
        "    # plt.title(f\"Label: {label_to_char[label]}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"# How many samples are there? \")\n",
        "print(f\"Number of training samples: {len(train_data)}\"),\n",
        "print(f\"Number of test samples: {len(test_data)}\"),\n",
        "print(len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets))\n",
        "\n",
        "class_names = train_data.classes\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWf29Je1Kxwh"
      },
      "source": [
        "# DataLoader in PyTorch\n",
        "\n",
        "torch.utils.data.DataLoader wraps your Dataset and splits it into mini-batches.\n",
        "\n",
        "We can shuffle the data each epoch by setting shuffle=True.\n",
        "\n",
        "The batch_size parameter controls how many samples per batch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PLuIBQ4KFBi"
      },
      "outputs": [],
      "source": [
        "## DATA LOADER IN PYTORCH\n",
        "print(\"DATA LOADER IN PYTORCH\")\n",
        "\n",
        "# BATCH SIZE\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# TURN THE DATASETS INTO ITERABLES(BATCHES)\n",
        "train_dataloader = DataLoader(\n",
        "    train_data, # make it iterable\n",
        "    batch_size = BATCH_SIZE, # how many samples or images per batch?\n",
        "    shuffle = True # shuffle data for every epoch\n",
        "    )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    train_data, # make it iterable\n",
        "    batch_size = BATCH_SIZE, # how many samples or images per batch?\n",
        "    shuffle = False # shuffle data for every epoch\n",
        "    )\n",
        "\n",
        "# Check out what's inside the training dataloader\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "print(\"train_features_batch.shape | \", \"train_labels_batch.shape\")\n",
        "print(train_features_batch.shape,\" | \",  train_labels_batch.shape)\n",
        "\n",
        "# mannual seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# See classes\n",
        "class_names = train_data.classes\n",
        "print(class_names)\n",
        "\n",
        "\n",
        "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
        "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
        "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(\"Off\")\n",
        "print(f\"Image size: {img.shape}\")\n",
        "print(f\"Label: {label}, label size: {label.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By8dBChto_vP"
      },
      "source": [
        "## 3. Model O: Build a baseline Model\n",
        "A baseline model is one of the simplest models we can imagine.\n",
        "\n",
        "We use the baseline as a starting point and try to improve upon it with subsequent, more complicated models.\n",
        "\n",
        "nn.Flatten() layer to flatten the image features complex dimensions data, that means we are compressing tensor into a single vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbdAs7qmphZZ"
      },
      "outputs": [],
      "source": [
        "# flatten layer\n",
        "flatten_model = nn.Flatten()\n",
        "\n",
        "# single sample\n",
        "x = train_features_batch[0]\n",
        "\n",
        "# flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "# print the shapes\n",
        "print(f\"shape before flattening: {x.shape} --> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} --> [color_channels, height** width]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxKRHJICskRY"
      },
      "source": [
        "### Model Creation for Alphanumeric NN\n",
        "nn.Linear() layers like their input to be in the form of feature vectors\n",
        "#### AlphaNumericNN is baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx8vXHSZsuQe"
      },
      "outputs": [],
      "source": [
        "class AlphaNumericNN(nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape:int):  # type hinting input_shape: int means use int nothing else\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(),  # NN input in vector form like [color_channels, height*width]\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_stack(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8xAd6XxsLu-"
      },
      "source": [
        "\n",
        "Wonderful!\n",
        "\n",
        "We've got a baseline model class we can use, now let's instantiate a model.\n",
        "\n",
        "with parametersModel Setup Summary:\n",
        "input_shape = 784 → 28×28 image flattened.\n",
        "\n",
        "hidden_units = 10 → 10 neurons in the hidden layer.\n",
        "\n",
        "output_shape = len(class_names) → One output per class (e.g., 10 for digits 0–9).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GegIGz5JsYJV"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# model setup with input parameters\n",
        "model_0 = AlphaNumericNN(input_shape=784,  # one for every pixel (28x28)\n",
        "                         hidden_units=10, # units in the hidden layer\n",
        "                         output_shape=len(class_names)\n",
        "                         )\n",
        "c = model_0.to(\"cpu\")\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyVmGY_Or2cX"
      },
      "source": [
        "### Setup Loss, Optimizer and Evaluation Metrics\n",
        "- using helper functions\n",
        "- can use evaluation metrics from the TorchMetrics Package\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "DmhURnYTrY8l",
        "outputId": "5b7dcd7b-8b07-4594-a3f0-29d4fc4e4bff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading helper_functions.py\n",
            "CrossEntropyLoss()\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'model_0' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a7d2d158bdf2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# also called cirterion or cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Learning rate =0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_0' is not defined"
          ]
        }
      ],
      "source": [
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  # Note: you need the \"raw\" GitHub URL for this to work\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss() # also called cirterion or cost\n",
        "print(loss_fn)\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1) #Learning rate =0.1\n",
        "print(optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVJnQu8atErf"
      },
      "source": [
        "## Creating a function to time our experiments\n",
        "Loss Functions and optimizer ready--->start training --> measure training time?? using built in function from the python module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "yOwFGt-xtP9H",
        "outputId": "c8723058-1fa2-41fd-fa3f-aef1ee857702"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-2-b666a27df3ee>, line 12)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-b666a27df3ee>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    total time = end- start\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "  \"\"\"\"\n",
        "  Prints the difference between start and end time.\n",
        "      Args:\n",
        "        start (float): Start time of computation (preferred in timeit format).\n",
        "        end (float): End time of computation.\n",
        "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: time between start and end in seconds (higher is longer).\n",
        "  \"\"\"\n",
        "  total time = end- start\n",
        "  print(f\"Train time on {device}: {total_time:3f} seconds\")\n",
        "  return total_time\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmcMGiL_t2TF"
      },
      "source": [
        "### Creating a training loop and training model on baatches of data\n",
        "\n",
        "- Data batches contained within Dataloaders, train_dataloader and test_dataloader for training and testing data splits repectively\n",
        "- a batchSIZE of X an y if using 32 batch size means 32 sample of iamges and targets\n",
        "- And since we're computing on batches of data, our loss and evaluation metrics will be calculated per batch rather than across the whole dataset.\n",
        "\n",
        "This means we'll have to divide our loss and accuracy values by the number of batches in each dataset's respective dataloader.\n",
        "\n",
        "\n",
        "Let's step through it:\n",
        "\n",
        "-Loop through epochs.\n",
        "- Loop through training batches, perform training steps, calculate the train loss per batch.\n",
        "- Loop through testing batches, perform testing steps, calculate the test loss per batch.\n",
        "- Print out what's happening.\n",
        "- Time it all (for fun)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "da98f92514b14a0986842d64e673857b",
            "400d2dd41cfe4fc3bded697710eb7eb9",
            "c41ba7fcec6246e2854fed1b99f8b996",
            "83faee5101044db8a0bb5e55cc1e6fe6",
            "a40ec0c63a5040e894758ee22aef5137",
            "2ddf158236d54ff6b8f378714d770c8c",
            "a70aec9a889b41e595b633a22ac3e4ef",
            "0f2bb8f4529b435dafc4bcf4d840854c",
            "5edfbfa822e64a4ca2def088952d46f3",
            "4a89bb6a8a734e31a2ca25005a861516",
            "7bf4bd4c2e8a443bb5d4a5057827a3b0"
          ]
        },
        "id": "B3Sv3TjuuiuH",
        "outputId": "a655e414-a0e5-4b57-9f07-510a86c99e53"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da98f92514b14a0986842d64e673857b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌀 Epoch: 0 ----- \n",
            "Looked at 0/ 112800 samples\n",
            "Train Loss: 0.00092 | Test Loss: 3.03259 | Test Acc: 20.58865\n",
            "Train Loss: 0.00097 | Test Loss: 3.03069 | Test Acc: 20.47074\n",
            "Train Loss: 0.00083 | Test Loss: 3.01873 | Test Acc: 20.41046\n",
            "Train Loss: 0.00086 | Test Loss: 3.01465 | Test Acc: 20.23493\n",
            "Train Loss: 0.00085 | Test Loss: 3.00203 | Test Acc: 20.49645\n",
            "Train Loss: 0.00087 | Test Loss: 2.99219 | Test Acc: 20.04433\n",
            "Train Loss: 0.00079 | Test Loss: 2.98808 | Test Acc: 20.01064\n",
            "Train Loss: 0.00085 | Test Loss: 2.98204 | Test Acc: 20.24202\n",
            "Train Loss: 0.00076 | Test Loss: 2.97139 | Test Acc: 21.66135\n",
            "Train Loss: 0.00082 | Test Loss: 2.95896 | Test Acc: 21.55851\n",
            "Train Loss: 0.00083 | Test Loss: 2.95283 | Test Acc: 22.18351\n",
            "Train Loss: 0.00081 | Test Loss: 2.95000 | Test Acc: 21.65071\n",
            "Train Loss: 0.00080 | Test Loss: 2.95512 | Test Acc: 21.59840\n",
            "Train Loss: 0.00087 | Test Loss: 2.94821 | Test Acc: 21.30762\n",
            "Train Loss: 0.00089 | Test Loss: 2.92631 | Test Acc: 22.05408\n",
            "Train Loss: 0.00089 | Test Loss: 2.93159 | Test Acc: 22.69681\n",
            "Train Loss: 0.00086 | Test Loss: 2.91499 | Test Acc: 23.25709\n",
            "Train Loss: 0.00089 | Test Loss: 2.89971 | Test Acc: 23.41755\n",
            "Train Loss: 0.00080 | Test Loss: 2.88640 | Test Acc: 23.41223\n",
            "Train Loss: 0.00083 | Test Loss: 2.88183 | Test Acc: 23.89184\n",
            "Train Loss: 0.00073 | Test Loss: 2.87870 | Test Acc: 23.80230\n",
            "Train Loss: 0.00079 | Test Loss: 2.86089 | Test Acc: 24.15071\n",
            "Train Loss: 0.00076 | Test Loss: 2.86106 | Test Acc: 23.80851\n",
            "Train Loss: 0.00073 | Test Loss: 2.84842 | Test Acc: 24.25798\n",
            "Train Loss: 0.00079 | Test Loss: 2.84257 | Test Acc: 24.25089\n",
            "Train Loss: 0.00070 | Test Loss: 2.84385 | Test Acc: 24.05230\n",
            "Train Loss: 0.00079 | Test Loss: 2.84077 | Test Acc: 24.22429\n",
            "Train Loss: 0.00080 | Test Loss: 2.82691 | Test Acc: 24.07004\n",
            "Train Loss: 0.00079 | Test Loss: 2.81662 | Test Acc: 24.99468\n",
            "Train Loss: 0.00074 | Test Loss: 2.80485 | Test Acc: 25.10904\n",
            "Train Loss: 0.00078 | Test Loss: 2.79084 | Test Acc: 25.98848\n",
            "Train Loss: 0.00080 | Test Loss: 2.78056 | Test Acc: 26.50621\n",
            "Train Loss: 0.00079 | Test Loss: 2.76766 | Test Acc: 26.72074\n",
            "Train Loss: 0.00077 | Test Loss: 2.75919 | Test Acc: 26.62411\n",
            "Train Loss: 0.00081 | Test Loss: 2.75701 | Test Acc: 26.42287\n",
            "Train Loss: 0.00078 | Test Loss: 2.77646 | Test Acc: 25.35372\n",
            "Train Loss: 0.00079 | Test Loss: 2.76730 | Test Acc: 25.64184\n",
            "Train Loss: 0.00074 | Test Loss: 2.79274 | Test Acc: 24.30585\n",
            "Train Loss: 0.00078 | Test Loss: 2.76177 | Test Acc: 25.24645\n",
            "Train Loss: 0.00081 | Test Loss: 2.71971 | Test Acc: 27.27305\n",
            "Train Loss: 0.00072 | Test Loss: 2.70853 | Test Acc: 27.22695\n",
            "Train Loss: 0.00073 | Test Loss: 2.70328 | Test Acc: 27.19858\n",
            "Train Loss: 0.00079 | Test Loss: 2.69402 | Test Acc: 28.01064\n",
            "Train Loss: 0.00087 | Test Loss: 2.69322 | Test Acc: 28.14628\n",
            "Train Loss: 0.00074 | Test Loss: 2.68220 | Test Acc: 28.50887\n",
            "Train Loss: 0.00082 | Test Loss: 2.67700 | Test Acc: 28.57270\n",
            "Train Loss: 0.00070 | Test Loss: 2.67863 | Test Acc: 28.33245\n",
            "Train Loss: 0.00070 | Test Loss: 2.66076 | Test Acc: 28.60106\n",
            "Train Loss: 0.00076 | Test Loss: 2.66030 | Test Acc: 28.16755\n",
            "Train Loss: 0.00072 | Test Loss: 2.64648 | Test Acc: 28.65869\n",
            "Train Loss: 0.00075 | Test Loss: 2.64047 | Test Acc: 28.09929\n",
            "Train Loss: 0.00068 | Test Loss: 2.63356 | Test Acc: 28.54787\n",
            "Train Loss: 0.00084 | Test Loss: 2.62264 | Test Acc: 29.20567\n",
            "Train Loss: 0.00076 | Test Loss: 2.63818 | Test Acc: 28.34752\n",
            "Train Loss: 0.00068 | Test Loss: 2.62084 | Test Acc: 28.59752\n",
            "Train Loss: 0.00071 | Test Loss: 2.61875 | Test Acc: 28.20035\n",
            "Train Loss: 0.00070 | Test Loss: 2.60423 | Test Acc: 28.53103\n",
            "Train Loss: 0.00078 | Test Loss: 2.59606 | Test Acc: 29.41223\n",
            "Train Loss: 0.00069 | Test Loss: 2.59019 | Test Acc: 29.23227\n",
            "Train Loss: 0.00073 | Test Loss: 2.57893 | Test Acc: 29.65071\n",
            "Train Loss: 0.00059 | Test Loss: 2.56391 | Test Acc: 30.55851\n",
            "Train Loss: 0.00069 | Test Loss: 2.55678 | Test Acc: 30.82004\n",
            "Train Loss: 0.00080 | Test Loss: 2.54742 | Test Acc: 30.93794\n",
            "Train Loss: 0.00069 | Test Loss: 2.54878 | Test Acc: 31.18617\n",
            "Train Loss: 0.00070 | Test Loss: 2.55407 | Test Acc: 30.71365\n",
            "Train Loss: 0.00073 | Test Loss: 2.55164 | Test Acc: 30.32181\n",
            "Train Loss: 0.00070 | Test Loss: 2.54111 | Test Acc: 31.12500\n",
            "Train Loss: 0.00081 | Test Loss: 2.52426 | Test Acc: 32.19149\n",
            "Train Loss: 0.00076 | Test Loss: 2.51013 | Test Acc: 33.57004\n",
            "Train Loss: 0.00075 | Test Loss: 2.51971 | Test Acc: 31.60018\n",
            "Train Loss: 0.00072 | Test Loss: 2.52232 | Test Acc: 31.33245\n",
            "Train Loss: 0.00066 | Test Loss: 2.50950 | Test Acc: 32.02748\n",
            "Train Loss: 0.00075 | Test Loss: 2.49075 | Test Acc: 33.22695\n",
            "Train Loss: 0.00070 | Test Loss: 2.47964 | Test Acc: 33.68528\n",
            "Train Loss: 0.00075 | Test Loss: 2.48011 | Test Acc: 33.69681\n",
            "Train Loss: 0.00065 | Test Loss: 2.46658 | Test Acc: 34.38387\n",
            "Train Loss: 0.00067 | Test Loss: 2.45527 | Test Acc: 34.36170\n",
            "Train Loss: 0.00066 | Test Loss: 2.44791 | Test Acc: 34.70922\n",
            "Train Loss: 0.00077 | Test Loss: 2.44135 | Test Acc: 34.72252\n",
            "Train Loss: 0.00071 | Test Loss: 2.42921 | Test Acc: 34.82624\n",
            "Train Loss: 0.00070 | Test Loss: 2.42136 | Test Acc: 34.81826\n",
            "Train Loss: 0.00074 | Test Loss: 2.42382 | Test Acc: 34.21809\n",
            "Train Loss: 0.00064 | Test Loss: 2.41210 | Test Acc: 35.11170\n",
            "Train Loss: 0.00068 | Test Loss: 2.41815 | Test Acc: 34.74734\n",
            "Train Loss: 0.00062 | Test Loss: 2.40183 | Test Acc: 35.62677\n",
            "Train Loss: 0.00066 | Test Loss: 2.40099 | Test Acc: 35.83688\n",
            "Train Loss: 0.00058 | Test Loss: 2.39183 | Test Acc: 36.29344\n",
            "Train Loss: 0.00070 | Test Loss: 2.38612 | Test Acc: 36.51330\n",
            "Train Loss: 0.00066 | Test Loss: 2.39139 | Test Acc: 36.00887\n",
            "Train Loss: 0.00071 | Test Loss: 2.38201 | Test Acc: 36.11613\n",
            "Train Loss: 0.00065 | Test Loss: 2.38604 | Test Acc: 35.49291\n",
            "Train Loss: 0.00073 | Test Loss: 2.37696 | Test Acc: 35.62057\n",
            "Train Loss: 0.00066 | Test Loss: 2.36436 | Test Acc: 36.23670\n",
            "Train Loss: 0.00068 | Test Loss: 2.35107 | Test Acc: 36.47961\n",
            "Train Loss: 0.00053 | Test Loss: 2.35163 | Test Acc: 36.17110\n",
            "Train Loss: 0.00067 | Test Loss: 2.36641 | Test Acc: 35.76507\n",
            "Train Loss: 0.00081 | Test Loss: 2.35559 | Test Acc: 36.14007\n",
            "Train Loss: 0.00072 | Test Loss: 2.34717 | Test Acc: 35.79433\n",
            "Train Loss: 0.00066 | Test Loss: 2.32873 | Test Acc: 37.10904\n",
            "Train Loss: 0.00078 | Test Loss: 2.33570 | Test Acc: 36.74468\n",
            "Train Loss: 0.00066 | Test Loss: 2.34085 | Test Acc: 36.26064\n",
            "Train Loss: 0.00060 | Test Loss: 2.33685 | Test Acc: 36.30319\n",
            "Train Loss: 0.00063 | Test Loss: 2.32654 | Test Acc: 36.87855\n",
            "Train Loss: 0.00071 | Test Loss: 2.30326 | Test Acc: 38.34752\n",
            "Train Loss: 0.00074 | Test Loss: 2.30531 | Test Acc: 38.39273\n",
            "Train Loss: 0.00076 | Test Loss: 2.30352 | Test Acc: 38.52571\n",
            "Train Loss: 0.00066 | Test Loss: 2.29667 | Test Acc: 38.71631\n",
            "Train Loss: 0.00062 | Test Loss: 2.29660 | Test Acc: 38.80762\n",
            "Train Loss: 0.00063 | Test Loss: 2.27949 | Test Acc: 39.57890\n",
            "Train Loss: 0.00066 | Test Loss: 2.27755 | Test Acc: 39.40071\n",
            "Train Loss: 0.00066 | Test Loss: 2.27973 | Test Acc: 39.37677\n",
            "Train Loss: 0.00066 | Test Loss: 2.26588 | Test Acc: 40.03901\n",
            "Train Loss: 0.00060 | Test Loss: 2.26862 | Test Acc: 39.59043\n",
            "Train Loss: 0.00070 | Test Loss: 2.26042 | Test Acc: 39.53280\n",
            "Train Loss: 0.00065 | Test Loss: 2.25957 | Test Acc: 39.53812\n",
            "Train Loss: 0.00059 | Test Loss: 2.25797 | Test Acc: 39.42553\n",
            "Train Loss: 0.00073 | Test Loss: 2.24935 | Test Acc: 40.06738\n",
            "Train Loss: 0.00056 | Test Loss: 2.25210 | Test Acc: 39.82713\n",
            "Train Loss: 0.00055 | Test Loss: 2.23927 | Test Acc: 40.07890\n",
            "Train Loss: 0.00061 | Test Loss: 2.22983 | Test Acc: 40.47784\n",
            "Train Loss: 0.00058 | Test Loss: 2.21680 | Test Acc: 40.62145\n",
            "Train Loss: 0.00068 | Test Loss: 2.21706 | Test Acc: 40.22074\n",
            "Train Loss: 0.00071 | Test Loss: 2.22850 | Test Acc: 39.76241\n",
            "Train Loss: 0.00064 | Test Loss: 2.21876 | Test Acc: 40.54167\n",
            "Train Loss: 0.00057 | Test Loss: 2.22235 | Test Acc: 40.12677\n",
            "Train Loss: 0.00059 | Test Loss: 2.21578 | Test Acc: 40.09220\n",
            "Train Loss: 0.00066 | Test Loss: 2.21121 | Test Acc: 39.99291\n",
            "Train Loss: 0.00074 | Test Loss: 2.21306 | Test Acc: 39.61702\n",
            "Train Loss: 0.00048 | Test Loss: 2.19913 | Test Acc: 40.62234\n",
            "Train Loss: 0.00055 | Test Loss: 2.20188 | Test Acc: 40.29787\n",
            "Train Loss: 0.00055 | Test Loss: 2.19226 | Test Acc: 40.56028\n",
            "Train Loss: 0.00051 | Test Loss: 2.19616 | Test Acc: 40.27216\n",
            "Train Loss: 0.00071 | Test Loss: 2.17739 | Test Acc: 40.88387\n",
            "Train Loss: 0.00058 | Test Loss: 2.17643 | Test Acc: 40.62855\n",
            "Train Loss: 0.00061 | Test Loss: 2.17049 | Test Acc: 40.75621\n",
            "Train Loss: 0.00050 | Test Loss: 2.17005 | Test Acc: 40.63652\n",
            "Train Loss: 0.00057 | Test Loss: 2.16776 | Test Acc: 40.44770\n",
            "Train Loss: 0.00060 | Test Loss: 2.15768 | Test Acc: 41.23936\n",
            "Train Loss: 0.00064 | Test Loss: 2.15858 | Test Acc: 40.85195\n",
            "Train Loss: 0.00053 | Test Loss: 2.14511 | Test Acc: 41.37500\n",
            "Train Loss: 0.00068 | Test Loss: 2.15384 | Test Acc: 41.03635\n",
            "Train Loss: 0.00058 | Test Loss: 2.15456 | Test Acc: 40.65957\n",
            "Train Loss: 0.00064 | Test Loss: 2.14914 | Test Acc: 40.91489\n",
            "Train Loss: 0.00059 | Test Loss: 2.14886 | Test Acc: 40.72872\n",
            "Train Loss: 0.00063 | Test Loss: 2.13602 | Test Acc: 41.23936\n",
            "Train Loss: 0.00070 | Test Loss: 2.12702 | Test Acc: 41.85106\n",
            "Train Loss: 0.00071 | Test Loss: 2.12594 | Test Acc: 42.11436\n",
            "Train Loss: 0.00061 | Test Loss: 2.13209 | Test Acc: 42.11968\n",
            "Train Loss: 0.00063 | Test Loss: 2.12309 | Test Acc: 42.67730\n",
            "Train Loss: 0.00071 | Test Loss: 2.11292 | Test Acc: 42.58777\n",
            "Train Loss: 0.00061 | Test Loss: 2.13738 | Test Acc: 41.38652\n",
            "Train Loss: 0.00061 | Test Loss: 2.12317 | Test Acc: 41.77216\n",
            "Train Loss: 0.00056 | Test Loss: 2.12032 | Test Acc: 42.12411\n",
            "Train Loss: 0.00060 | Test Loss: 2.10605 | Test Acc: 42.54876\n",
            "Train Loss: 0.00056 | Test Loss: 2.10397 | Test Acc: 42.48670\n",
            "Train Loss: 0.00063 | Test Loss: 2.11736 | Test Acc: 41.75089\n",
            "Train Loss: 0.00062 | Test Loss: 2.09636 | Test Acc: 43.07713\n",
            "Train Loss: 0.00046 | Test Loss: 2.09699 | Test Acc: 42.81206\n",
            "Train Loss: 0.00070 | Test Loss: 2.11596 | Test Acc: 42.66046\n",
            "Train Loss: 0.00069 | Test Loss: 2.10151 | Test Acc: 43.10284\n",
            "Train Loss: 0.00054 | Test Loss: 2.11589 | Test Acc: 42.20301\n",
            "Train Loss: 0.00061 | Test Loss: 2.08444 | Test Acc: 43.19238\n",
            "Train Loss: 0.00061 | Test Loss: 2.07999 | Test Acc: 43.07535\n",
            "Train Loss: 0.00056 | Test Loss: 2.09137 | Test Acc: 42.77128\n",
            "Train Loss: 0.00050 | Test Loss: 2.07259 | Test Acc: 43.39539\n",
            "Train Loss: 0.00067 | Test Loss: 2.05309 | Test Acc: 44.06206\n",
            "Train Loss: 0.00052 | Test Loss: 2.05063 | Test Acc: 43.82979\n",
            "Train Loss: 0.00062 | Test Loss: 2.05605 | Test Acc: 43.54521\n",
            "Train Loss: 0.00059 | Test Loss: 2.05583 | Test Acc: 43.77039\n",
            "Train Loss: 0.00061 | Test Loss: 2.04253 | Test Acc: 44.36348\n",
            "Train Loss: 0.00062 | Test Loss: 2.04103 | Test Acc: 44.25532\n",
            "Train Loss: 0.00055 | Test Loss: 2.04136 | Test Acc: 44.25532\n",
            "Train Loss: 0.00060 | Test Loss: 2.04393 | Test Acc: 44.08156\n",
            "Train Loss: 0.00052 | Test Loss: 2.04678 | Test Acc: 43.71099\n",
            "Train Loss: 0.00048 | Test Loss: 2.04641 | Test Acc: 43.68174\n",
            "Train Loss: 0.00046 | Test Loss: 2.03499 | Test Acc: 44.08245\n",
            "Train Loss: 0.00063 | Test Loss: 2.04170 | Test Acc: 43.45833\n",
            "Train Loss: 0.00051 | Test Loss: 2.03393 | Test Acc: 44.02305\n",
            "Train Loss: 0.00048 | Test Loss: 2.02561 | Test Acc: 44.57358\n",
            "Train Loss: 0.00057 | Test Loss: 2.01534 | Test Acc: 45.25266\n",
            "Train Loss: 0.00062 | Test Loss: 2.01560 | Test Acc: 45.26773\n",
            "Train Loss: 0.00052 | Test Loss: 2.00466 | Test Acc: 45.71897\n",
            "Train Loss: 0.00051 | Test Loss: 2.00427 | Test Acc: 45.78635\n",
            "Train Loss: 0.00060 | Test Loss: 2.01880 | Test Acc: 45.13298\n",
            "Train Loss: 0.00053 | Test Loss: 2.01507 | Test Acc: 45.32535\n",
            "Train Loss: 0.00061 | Test Loss: 2.02442 | Test Acc: 45.17199\n",
            "Train Loss: 0.00057 | Test Loss: 2.00763 | Test Acc: 45.64628\n",
            "Train Loss: 0.00061 | Test Loss: 2.00254 | Test Acc: 45.70567\n",
            "Train Loss: 0.00061 | Test Loss: 1.99575 | Test Acc: 45.86525\n",
            "Train Loss: 0.00043 | Test Loss: 2.01306 | Test Acc: 45.11613\n",
            "Train Loss: 0.00057 | Test Loss: 2.01992 | Test Acc: 44.66578\n",
            "Train Loss: 0.00052 | Test Loss: 1.98901 | Test Acc: 45.56383\n",
            "Train Loss: 0.00058 | Test Loss: 1.98937 | Test Acc: 45.54787\n",
            "Train Loss: 0.00063 | Test Loss: 1.99419 | Test Acc: 45.66667\n",
            "Train Loss: 0.00043 | Test Loss: 2.00265 | Test Acc: 45.04876\n",
            "Train Loss: 0.00064 | Test Loss: 1.99629 | Test Acc: 45.29699\n",
            "Train Loss: 0.00060 | Test Loss: 1.97962 | Test Acc: 45.97429\n",
            "Train Loss: 0.00060 | Test Loss: 1.98037 | Test Acc: 45.44592\n",
            "Train Loss: 0.00062 | Test Loss: 1.97533 | Test Acc: 45.53457\n",
            "Train Loss: 0.00062 | Test Loss: 1.97783 | Test Acc: 45.65869\n",
            "Train Loss: 0.00044 | Test Loss: 1.98163 | Test Acc: 45.44681\n",
            "Train Loss: 0.00053 | Test Loss: 1.98588 | Test Acc: 45.10284\n",
            "Train Loss: 0.00057 | Test Loss: 1.97373 | Test Acc: 45.74025\n",
            "Train Loss: 0.00048 | Test Loss: 1.95456 | Test Acc: 46.39362\n",
            "Train Loss: 0.00056 | Test Loss: 1.96371 | Test Acc: 45.61968\n",
            "Train Loss: 0.00060 | Test Loss: 1.95915 | Test Acc: 45.73050\n",
            "Train Loss: 0.00052 | Test Loss: 1.95463 | Test Acc: 46.21631\n",
            "Train Loss: 0.00059 | Test Loss: 1.95821 | Test Acc: 46.31383\n",
            "Train Loss: 0.00049 | Test Loss: 1.95612 | Test Acc: 46.31383\n",
            "Train Loss: 0.00048 | Test Loss: 1.94400 | Test Acc: 46.83865\n",
            "Train Loss: 0.00066 | Test Loss: 1.94868 | Test Acc: 46.36613\n",
            "Train Loss: 0.00062 | Test Loss: 1.94716 | Test Acc: 46.38032\n",
            "Train Loss: 0.00049 | Test Loss: 1.95492 | Test Acc: 46.34486\n",
            "Train Loss: 0.00052 | Test Loss: 1.95094 | Test Acc: 46.36525\n",
            "Train Loss: 0.00050 | Test Loss: 1.97573 | Test Acc: 44.81117\n",
            "Train Loss: 0.00047 | Test Loss: 1.97055 | Test Acc: 45.00355\n",
            "Train Loss: 0.00060 | Test Loss: 1.97182 | Test Acc: 44.88121\n",
            "Train Loss: 0.00042 | Test Loss: 1.95182 | Test Acc: 45.71277\n",
            "Train Loss: 0.00066 | Test Loss: 1.93499 | Test Acc: 46.38298\n",
            "Train Loss: 0.00060 | Test Loss: 1.94548 | Test Acc: 45.92110\n",
            "Train Loss: 0.00059 | Test Loss: 1.96298 | Test Acc: 45.20745\n",
            "Train Loss: 0.00065 | Test Loss: 1.93672 | Test Acc: 46.27394\n",
            "Train Loss: 0.00061 | Test Loss: 1.94888 | Test Acc: 45.52039\n",
            "Train Loss: 0.00056 | Test Loss: 1.91549 | Test Acc: 46.73670\n",
            "Train Loss: 0.00042 | Test Loss: 1.92102 | Test Acc: 46.24379\n",
            "Train Loss: 0.00052 | Test Loss: 1.92825 | Test Acc: 45.74645\n",
            "Train Loss: 0.00061 | Test Loss: 1.91929 | Test Acc: 46.51152\n",
            "Train Loss: 0.00073 | Test Loss: 1.90747 | Test Acc: 47.24202\n",
            "Train Loss: 0.00049 | Test Loss: 1.91102 | Test Acc: 47.42642\n",
            "Train Loss: 0.00059 | Test Loss: 1.91749 | Test Acc: 47.37589\n",
            "Train Loss: 0.00051 | Test Loss: 1.92297 | Test Acc: 46.89273\n",
            "Train Loss: 0.00062 | Test Loss: 1.91005 | Test Acc: 47.36791\n",
            "Train Loss: 0.00050 | Test Loss: 1.90235 | Test Acc: 47.27748\n",
            "Train Loss: 0.00050 | Test Loss: 1.89821 | Test Acc: 47.46809\n",
            "Train Loss: 0.00066 | Test Loss: 1.90795 | Test Acc: 47.49291\n",
            "Train Loss: 0.00049 | Test Loss: 1.90171 | Test Acc: 47.51507\n",
            "Train Loss: 0.00063 | Test Loss: 1.89772 | Test Acc: 47.49645\n",
            "Train Loss: 0.00056 | Test Loss: 1.90178 | Test Acc: 47.72695\n",
            "Train Loss: 0.00057 | Test Loss: 1.88443 | Test Acc: 48.35018\n",
            "Train Loss: 0.00051 | Test Loss: 1.87628 | Test Acc: 48.59929\n",
            "Train Loss: 0.00053 | Test Loss: 1.87291 | Test Acc: 48.51330\n",
            "Train Loss: 0.00067 | Test Loss: 1.89337 | Test Acc: 47.50887\n",
            "Train Loss: 0.00055 | Test Loss: 1.87771 | Test Acc: 48.17110\n",
            "Train Loss: 0.00054 | Test Loss: 1.88309 | Test Acc: 47.44060\n",
            "Train Loss: 0.00059 | Test Loss: 1.87511 | Test Acc: 48.03546\n",
            "Train Loss: 0.00066 | Test Loss: 1.87754 | Test Acc: 48.11968\n",
            "Train Loss: 0.00069 | Test Loss: 1.87301 | Test Acc: 48.13475\n",
            "Train Loss: 0.00059 | Test Loss: 1.88108 | Test Acc: 47.96099\n",
            "Train Loss: 0.00043 | Test Loss: 1.87816 | Test Acc: 48.14184\n",
            "Train Loss: 0.00040 | Test Loss: 1.87691 | Test Acc: 48.03191\n",
            "Train Loss: 0.00055 | Test Loss: 1.86509 | Test Acc: 48.29344\n",
            "Train Loss: 0.00051 | Test Loss: 1.87864 | Test Acc: 48.02926\n",
            "Train Loss: 0.00043 | Test Loss: 1.87291 | Test Acc: 47.70567\n",
            "Train Loss: 0.00045 | Test Loss: 1.88238 | Test Acc: 47.02305\n",
            "Train Loss: 0.00059 | Test Loss: 1.88458 | Test Acc: 47.67730\n",
            "Train Loss: 0.00059 | Test Loss: 1.89433 | Test Acc: 47.62323\n",
            "Train Loss: 0.00045 | Test Loss: 1.86684 | Test Acc: 48.31028\n",
            "Train Loss: 0.00058 | Test Loss: 1.86364 | Test Acc: 48.58422\n",
            "Train Loss: 0.00052 | Test Loss: 1.85669 | Test Acc: 48.89894\n",
            "Train Loss: 0.00058 | Test Loss: 1.85912 | Test Acc: 48.76684\n",
            "Train Loss: 0.00047 | Test Loss: 1.85593 | Test Acc: 48.86436\n",
            "Train Loss: 0.00055 | Test Loss: 1.84309 | Test Acc: 49.03014\n",
            "Train Loss: 0.00046 | Test Loss: 1.84173 | Test Acc: 49.21454\n",
            "Train Loss: 0.00053 | Test Loss: 1.85045 | Test Acc: 48.76064\n",
            "Train Loss: 0.00043 | Test Loss: 1.85278 | Test Acc: 48.44238\n",
            "Train Loss: 0.00048 | Test Loss: 1.84843 | Test Acc: 48.58599\n",
            "Train Loss: 0.00037 | Test Loss: 1.87456 | Test Acc: 47.54167\n",
            "Train Loss: 0.00047 | Test Loss: 1.86653 | Test Acc: 47.49025\n",
            "Train Loss: 0.00059 | Test Loss: 1.87852 | Test Acc: 46.63741\n",
            "Train Loss: 0.00067 | Test Loss: 1.85241 | Test Acc: 47.63387\n",
            "Train Loss: 0.00052 | Test Loss: 1.84720 | Test Acc: 47.75887\n",
            "Train Loss: 0.00052 | Test Loss: 1.84437 | Test Acc: 47.91489\n",
            "Train Loss: 0.00061 | Test Loss: 1.84491 | Test Acc: 47.83067\n",
            "Train Loss: 0.00050 | Test Loss: 1.85307 | Test Acc: 47.24823\n",
            "Train Loss: 0.00058 | Test Loss: 1.84192 | Test Acc: 47.83422\n",
            "Train Loss: 0.00067 | Test Loss: 1.84468 | Test Acc: 47.74113\n",
            "Train Loss: 0.00052 | Test Loss: 1.83743 | Test Acc: 48.16135\n",
            "Train Loss: 0.00048 | Test Loss: 1.84275 | Test Acc: 47.78546\n",
            "Train Loss: 0.00053 | Test Loss: 1.84545 | Test Acc: 47.69858\n",
            "Train Loss: 0.00046 | Test Loss: 1.83442 | Test Acc: 48.32181\n",
            "Train Loss: 0.00048 | Test Loss: 1.84735 | Test Acc: 48.06826\n",
            "Train Loss: 0.00046 | Test Loss: 1.84258 | Test Acc: 48.56472\n",
            "Train Loss: 0.00048 | Test Loss: 1.81845 | Test Acc: 49.57358\n",
            "Train Loss: 0.00045 | Test Loss: 1.81086 | Test Acc: 49.82181\n",
            "Train Loss: 0.00057 | Test Loss: 1.81038 | Test Acc: 49.84043\n",
            "Train Loss: 0.00054 | Test Loss: 1.81756 | Test Acc: 49.38298\n",
            "Train Loss: 0.00060 | Test Loss: 1.80987 | Test Acc: 49.78191\n",
            "Train Loss: 0.00048 | Test Loss: 1.81187 | Test Acc: 49.75975\n",
            "Train Loss: 0.00061 | Test Loss: 1.81193 | Test Acc: 49.19858\n",
            "Train Loss: 0.00057 | Test Loss: 1.81041 | Test Acc: 49.32890\n",
            "Train Loss: 0.00056 | Test Loss: 1.81971 | Test Acc: 49.06472\n",
            "Train Loss: 0.00045 | Test Loss: 1.81336 | Test Acc: 48.90603\n",
            "Train Loss: 0.00046 | Test Loss: 1.80401 | Test Acc: 49.54876\n",
            "Train Loss: 0.00041 | Test Loss: 1.79981 | Test Acc: 50.15869\n",
            "Train Loss: 0.00049 | Test Loss: 1.80751 | Test Acc: 49.92819\n",
            "Train Loss: 0.00047 | Test Loss: 1.81880 | Test Acc: 49.08067\n",
            "Train Loss: 0.00046 | Test Loss: 1.81532 | Test Acc: 49.18351\n",
            "Train Loss: 0.00042 | Test Loss: 1.79986 | Test Acc: 49.90071\n",
            "Train Loss: 0.00052 | Test Loss: 1.81435 | Test Acc: 48.74734\n",
            "Train Loss: 0.00043 | Test Loss: 1.80990 | Test Acc: 49.35550\n",
            "Train Loss: 0.00055 | Test Loss: 1.79917 | Test Acc: 49.88475\n",
            "Train Loss: 0.00038 | Test Loss: 1.78690 | Test Acc: 50.44592\n",
            "Train Loss: 0.00049 | Test Loss: 1.78546 | Test Acc: 50.75089\n",
            "Train Loss: 0.00034 | Test Loss: 1.79291 | Test Acc: 50.39362\n",
            "Train Loss: 0.00071 | Test Loss: 1.79153 | Test Acc: 50.25621\n",
            "Train Loss: 0.00058 | Test Loss: 1.78543 | Test Acc: 50.67021\n",
            "Train Loss: 0.00046 | Test Loss: 1.78771 | Test Acc: 50.09043\n",
            "Train Loss: 0.00054 | Test Loss: 1.79153 | Test Acc: 50.32181\n",
            "Train Loss: 0.00058 | Test Loss: 1.80939 | Test Acc: 49.69858\n",
            "Train Loss: 0.00048 | Test Loss: 1.79215 | Test Acc: 50.35727\n",
            "Train Loss: 0.00056 | Test Loss: 1.78051 | Test Acc: 50.64273\n",
            "Train Loss: 0.00042 | Test Loss: 1.79020 | Test Acc: 50.24557\n",
            "Train Loss: 0.00046 | Test Loss: 1.78289 | Test Acc: 50.73316\n",
            "Train Loss: 0.00048 | Test Loss: 1.80877 | Test Acc: 49.75266\n",
            "Train Loss: 0.00051 | Test Loss: 1.79845 | Test Acc: 49.53191\n",
            "Train Loss: 0.00047 | Test Loss: 1.78222 | Test Acc: 50.02394\n",
            "Train Loss: 0.00050 | Test Loss: 1.78804 | Test Acc: 49.90780\n",
            "Train Loss: 0.00052 | Test Loss: 1.79689 | Test Acc: 49.43794\n",
            "Train Loss: 0.00045 | Test Loss: 1.78894 | Test Acc: 49.70567\n",
            "Train Loss: 0.00050 | Test Loss: 1.78617 | Test Acc: 49.82092\n",
            "Train Loss: 0.00059 | Test Loss: 1.78728 | Test Acc: 49.27571\n",
            "Train Loss: 0.00045 | Test Loss: 1.79123 | Test Acc: 49.17021\n",
            "Train Loss: 0.00047 | Test Loss: 1.77887 | Test Acc: 50.09574\n",
            "Train Loss: 0.00060 | Test Loss: 1.77697 | Test Acc: 50.21897\n",
            "Train Loss: 0.00044 | Test Loss: 1.77387 | Test Acc: 50.54521\n",
            "Train Loss: 0.00070 | Test Loss: 1.78275 | Test Acc: 50.14096\n",
            "Train Loss: 0.00048 | Test Loss: 1.79167 | Test Acc: 50.09840\n",
            "Train Loss: 0.00043 | Test Loss: 1.79748 | Test Acc: 49.67908\n",
            "Train Loss: 0.00039 | Test Loss: 1.76887 | Test Acc: 50.79433\n",
            "Train Loss: 0.00057 | Test Loss: 1.79983 | Test Acc: 49.58688\n",
            "Train Loss: 0.00050 | Test Loss: 1.78618 | Test Acc: 49.84929\n",
            "Train Loss: 0.00047 | Test Loss: 1.78835 | Test Acc: 49.76596\n",
            "Train Loss: 0.00055 | Test Loss: 1.76808 | Test Acc: 50.42642\n",
            "Train Loss: 0.00056 | Test Loss: 1.76791 | Test Acc: 50.33422\n",
            "Train Loss: 0.00040 | Test Loss: 1.77573 | Test Acc: 50.21720\n",
            "Train Loss: 0.00044 | Test Loss: 1.77689 | Test Acc: 50.45922\n",
            "Train Loss: 0.00042 | Test Loss: 1.77280 | Test Acc: 50.52926\n",
            "Train Loss: 0.00052 | Test Loss: 1.79175 | Test Acc: 49.75355\n",
            "Train Loss: 0.00040 | Test Loss: 1.78310 | Test Acc: 50.33156\n",
            "Train Loss: 0.00041 | Test Loss: 1.78226 | Test Acc: 50.23404\n",
            "Train Loss: 0.00051 | Test Loss: 1.76914 | Test Acc: 50.28546\n",
            "Train Loss: 0.00055 | Test Loss: 1.75887 | Test Acc: 50.63652\n",
            "Train Loss: 0.00048 | Test Loss: 1.75323 | Test Acc: 51.01330\n",
            "Train Loss: 0.00057 | Test Loss: 1.75530 | Test Acc: 51.21365\n",
            "Train Loss: 0.00046 | Test Loss: 1.75774 | Test Acc: 50.91046\n",
            "Train Loss: 0.00050 | Test Loss: 1.74917 | Test Acc: 51.05053\n",
            "Train Loss: 0.00047 | Test Loss: 1.75474 | Test Acc: 50.93528\n",
            "Train Loss: 0.00059 | Test Loss: 1.74677 | Test Acc: 51.02216\n",
            "Train Loss: 0.00041 | Test Loss: 1.73913 | Test Acc: 51.49025\n",
            "Train Loss: 0.00052 | Test Loss: 1.76753 | Test Acc: 50.18972\n",
            "Train Loss: 0.00045 | Test Loss: 1.74519 | Test Acc: 51.05319\n",
            "Train Loss: 0.00046 | Test Loss: 1.73629 | Test Acc: 51.82447\n",
            "Train Loss: 0.00046 | Test Loss: 1.74228 | Test Acc: 51.62500\n",
            "Train Loss: 0.00047 | Test Loss: 1.74725 | Test Acc: 51.42110\n",
            "Train Loss: 0.00062 | Test Loss: 1.73047 | Test Acc: 51.90869\n",
            "Train Loss: 0.00047 | Test Loss: 1.73699 | Test Acc: 51.83245\n",
            "Train Loss: 0.00064 | Test Loss: 1.74617 | Test Acc: 51.48936\n",
            "Train Loss: 0.00042 | Test Loss: 1.74002 | Test Acc: 51.99557\n",
            "Train Loss: 0.00055 | Test Loss: 1.74525 | Test Acc: 51.89628\n",
            "Train Loss: 0.00050 | Test Loss: 1.75145 | Test Acc: 51.82890\n",
            "Train Loss: 0.00052 | Test Loss: 1.73555 | Test Acc: 52.29078\n",
            "Train Loss: 0.00063 | Test Loss: 1.72872 | Test Acc: 52.42819\n",
            "Train Loss: 0.00056 | Test Loss: 1.73489 | Test Acc: 52.10816\n",
            "Train Loss: 0.00054 | Test Loss: 1.74770 | Test Acc: 51.56915\n",
            "Train Loss: 0.00058 | Test Loss: 1.73192 | Test Acc: 52.26596\n",
            "Train Loss: 0.00048 | Test Loss: 1.71666 | Test Acc: 52.59663\n",
            "Train Loss: 0.00040 | Test Loss: 1.72824 | Test Acc: 51.90869\n",
            "Train Loss: 0.00058 | Test Loss: 1.72759 | Test Acc: 51.78635\n",
            "Train Loss: 0.00044 | Test Loss: 1.72150 | Test Acc: 51.58511\n",
            "Train Loss: 0.00047 | Test Loss: 1.72245 | Test Acc: 51.93440\n",
            "Train Loss: 0.00041 | Test Loss: 1.72181 | Test Acc: 51.81472\n",
            "Train Loss: 0.00055 | Test Loss: 1.71966 | Test Acc: 51.78812\n",
            "Train Loss: 0.00043 | Test Loss: 1.72511 | Test Acc: 51.66489\n",
            "Train Loss: 0.00054 | Test Loss: 1.74137 | Test Acc: 50.86791\n",
            "Train Loss: 0.00049 | Test Loss: 1.72821 | Test Acc: 51.51418\n",
            "Train Loss: 0.00055 | Test Loss: 1.73858 | Test Acc: 51.25443\n",
            "Train Loss: 0.00044 | Test Loss: 1.72921 | Test Acc: 51.63830\n",
            "Train Loss: 0.00050 | Test Loss: 1.72448 | Test Acc: 51.90957\n",
            "Train Loss: 0.00052 | Test Loss: 1.72965 | Test Acc: 51.73316\n",
            "Train Loss: 0.00050 | Test Loss: 1.72569 | Test Acc: 51.78457\n",
            "Train Loss: 0.00041 | Test Loss: 1.74232 | Test Acc: 50.97961\n",
            "Train Loss: 0.00060 | Test Loss: 1.74049 | Test Acc: 51.24911\n",
            "Train Loss: 0.00044 | Test Loss: 1.75317 | Test Acc: 50.59840\n",
            "Train Loss: 0.00033 | Test Loss: 1.73505 | Test Acc: 51.40337\n",
            "Train Loss: 0.00052 | Test Loss: 1.74026 | Test Acc: 51.45124\n",
            "Train Loss: 0.00029 | Test Loss: 1.73607 | Test Acc: 51.70833\n",
            "Train Loss: 0.00042 | Test Loss: 1.72099 | Test Acc: 51.96011\n",
            "Train Loss: 0.00067 | Test Loss: 1.71450 | Test Acc: 52.37677\n",
            "Train Loss: 0.00043 | Test Loss: 1.72268 | Test Acc: 52.21543\n",
            "Train Loss: 0.00044 | Test Loss: 1.71391 | Test Acc: 52.37766\n",
            "Train Loss: 0.00057 | Test Loss: 1.72077 | Test Acc: 51.88475\n",
            "Train Loss: 0.00053 | Test Loss: 1.72585 | Test Acc: 51.61968\n",
            "Train Loss: 0.00037 | Test Loss: 1.75073 | Test Acc: 50.75798\n",
            "Train Loss: 0.00062 | Test Loss: 1.75452 | Test Acc: 50.65603\n",
            "Train Loss: 0.00056 | Test Loss: 1.72667 | Test Acc: 51.65957\n",
            "Train Loss: 0.00052 | Test Loss: 1.71076 | Test Acc: 52.22961\n",
            "Train Loss: 0.00045 | Test Loss: 1.72666 | Test Acc: 51.21454\n",
            "Train Loss: 0.00038 | Test Loss: 1.73168 | Test Acc: 51.08511\n",
            "Train Loss: 0.00054 | Test Loss: 1.71856 | Test Acc: 51.68883\n",
            "Train Loss: 0.00045 | Test Loss: 1.70864 | Test Acc: 51.91223\n",
            "Looked at 12800/ 112800 samples\n",
            "Train Loss: 0.00047 | Test Loss: 1.70814 | Test Acc: 52.13741\n",
            "Train Loss: 0.00049 | Test Loss: 1.69435 | Test Acc: 52.72518\n",
            "Train Loss: 0.00045 | Test Loss: 1.70807 | Test Acc: 52.07979\n",
            "Train Loss: 0.00048 | Test Loss: 1.69655 | Test Acc: 53.01950\n",
            "Train Loss: 0.00077 | Test Loss: 1.71449 | Test Acc: 52.44947\n",
            "Train Loss: 0.00054 | Test Loss: 1.71067 | Test Acc: 52.34220\n",
            "Train Loss: 0.00057 | Test Loss: 1.70751 | Test Acc: 52.59043\n",
            "Train Loss: 0.00044 | Test Loss: 1.69963 | Test Acc: 52.14362\n",
            "Train Loss: 0.00056 | Test Loss: 1.71321 | Test Acc: 52.23227\n",
            "Train Loss: 0.00043 | Test Loss: 1.70410 | Test Acc: 52.30230\n",
            "Train Loss: 0.00041 | Test Loss: 1.70462 | Test Acc: 52.34929\n",
            "Train Loss: 0.00041 | Test Loss: 1.71004 | Test Acc: 51.95035\n",
            "Train Loss: 0.00062 | Test Loss: 1.70108 | Test Acc: 52.51241\n",
            "Train Loss: 0.00051 | Test Loss: 1.71639 | Test Acc: 51.58599\n",
            "Train Loss: 0.00044 | Test Loss: 1.72402 | Test Acc: 51.28901\n",
            "Train Loss: 0.00044 | Test Loss: 1.70010 | Test Acc: 52.30230\n",
            "Train Loss: 0.00055 | Test Loss: 1.69349 | Test Acc: 52.63032\n",
            "Train Loss: 0.00046 | Test Loss: 1.69357 | Test Acc: 52.54433\n",
            "Train Loss: 0.00048 | Test Loss: 1.69564 | Test Acc: 52.58954\n",
            "Train Loss: 0.00042 | Test Loss: 1.69893 | Test Acc: 52.50798\n",
            "Train Loss: 0.00048 | Test Loss: 1.68756 | Test Acc: 52.75443\n",
            "Train Loss: 0.00045 | Test Loss: 1.69452 | Test Acc: 52.44770\n",
            "Train Loss: 0.00057 | Test Loss: 1.68293 | Test Acc: 53.01241\n",
            "Train Loss: 0.00045 | Test Loss: 1.68785 | Test Acc: 52.51064\n",
            "Train Loss: 0.00042 | Test Loss: 1.68057 | Test Acc: 52.46011\n",
            "Train Loss: 0.00050 | Test Loss: 1.68167 | Test Acc: 52.43262\n",
            "Train Loss: 0.00044 | Test Loss: 1.69824 | Test Acc: 52.03191\n",
            "Train Loss: 0.00044 | Test Loss: 1.68912 | Test Acc: 52.60284\n",
            "Train Loss: 0.00041 | Test Loss: 1.69914 | Test Acc: 52.41844\n",
            "Train Loss: 0.00062 | Test Loss: 1.67616 | Test Acc: 52.91489\n",
            "Train Loss: 0.00046 | Test Loss: 1.67143 | Test Acc: 53.04787\n",
            "Train Loss: 0.00049 | Test Loss: 1.67026 | Test Acc: 53.23582\n",
            "Train Loss: 0.00060 | Test Loss: 1.66204 | Test Acc: 53.30940\n",
            "Train Loss: 0.00058 | Test Loss: 1.66938 | Test Acc: 53.25798\n",
            "Train Loss: 0.00042 | Test Loss: 1.69066 | Test Acc: 52.79521\n",
            "Train Loss: 0.00038 | Test Loss: 1.70562 | Test Acc: 52.32181\n",
            "Train Loss: 0.00043 | Test Loss: 1.68945 | Test Acc: 52.62411\n",
            "Train Loss: 0.00059 | Test Loss: 1.68131 | Test Acc: 52.79344\n",
            "Train Loss: 0.00040 | Test Loss: 1.67232 | Test Acc: 53.01507\n",
            "Train Loss: 0.00039 | Test Loss: 1.66891 | Test Acc: 53.23227\n",
            "Train Loss: 0.00050 | Test Loss: 1.67430 | Test Acc: 53.14628\n",
            "Train Loss: 0.00051 | Test Loss: 1.66871 | Test Acc: 53.37589\n",
            "Train Loss: 0.00043 | Test Loss: 1.66538 | Test Acc: 53.57270\n",
            "Train Loss: 0.00054 | Test Loss: 1.67314 | Test Acc: 53.14539\n",
            "Train Loss: 0.00040 | Test Loss: 1.67297 | Test Acc: 53.06028\n",
            "Train Loss: 0.00041 | Test Loss: 1.67738 | Test Acc: 53.21897\n",
            "Train Loss: 0.00040 | Test Loss: 1.66316 | Test Acc: 53.58067\n",
            "Train Loss: 0.00048 | Test Loss: 1.66588 | Test Acc: 53.21188\n",
            "Train Loss: 0.00050 | Test Loss: 1.65157 | Test Acc: 53.91489\n",
            "Train Loss: 0.00050 | Test Loss: 1.66262 | Test Acc: 53.21720\n",
            "Train Loss: 0.00054 | Test Loss: 1.66565 | Test Acc: 52.91135\n",
            "Train Loss: 0.00048 | Test Loss: 1.68043 | Test Acc: 52.48138\n",
            "Train Loss: 0.00038 | Test Loss: 1.66763 | Test Acc: 53.17199\n",
            "Train Loss: 0.00040 | Test Loss: 1.67358 | Test Acc: 52.65426\n",
            "Train Loss: 0.00042 | Test Loss: 1.67502 | Test Acc: 52.28457\n",
            "Train Loss: 0.00046 | Test Loss: 1.67141 | Test Acc: 52.49645\n",
            "Train Loss: 0.00046 | Test Loss: 1.68302 | Test Acc: 51.84220\n",
            "Train Loss: 0.00049 | Test Loss: 1.67251 | Test Acc: 52.14894\n",
            "Train Loss: 0.00060 | Test Loss: 1.68046 | Test Acc: 51.96277\n",
            "Train Loss: 0.00043 | Test Loss: 1.67299 | Test Acc: 52.43351\n",
            "Train Loss: 0.00056 | Test Loss: 1.68776 | Test Acc: 52.12766\n",
            "Train Loss: 0.00044 | Test Loss: 1.67200 | Test Acc: 53.31028\n",
            "Train Loss: 0.00038 | Test Loss: 1.66605 | Test Acc: 53.41755\n",
            "Train Loss: 0.00039 | Test Loss: 1.66734 | Test Acc: 53.27216\n",
            "Train Loss: 0.00041 | Test Loss: 1.67262 | Test Acc: 52.89894\n",
            "Train Loss: 0.00043 | Test Loss: 1.65641 | Test Acc: 53.83422\n",
            "Train Loss: 0.00055 | Test Loss: 1.64725 | Test Acc: 53.83067\n",
            "Train Loss: 0.00054 | Test Loss: 1.65475 | Test Acc: 53.81294\n",
            "Train Loss: 0.00030 | Test Loss: 1.64661 | Test Acc: 53.83599\n",
            "Train Loss: 0.00055 | Test Loss: 1.65343 | Test Acc: 53.78014\n",
            "Train Loss: 0.00050 | Test Loss: 1.65124 | Test Acc: 53.82713\n",
            "Train Loss: 0.00039 | Test Loss: 1.65843 | Test Acc: 52.98936\n",
            "Train Loss: 0.00037 | Test Loss: 1.65000 | Test Acc: 53.07270\n",
            "Train Loss: 0.00036 | Test Loss: 1.65401 | Test Acc: 52.96720\n",
            "Train Loss: 0.00050 | Test Loss: 1.67430 | Test Acc: 52.76330\n",
            "Train Loss: 0.00057 | Test Loss: 1.65483 | Test Acc: 53.18883\n",
            "Train Loss: 0.00037 | Test Loss: 1.64331 | Test Acc: 53.55230\n",
            "Train Loss: 0.00035 | Test Loss: 1.64171 | Test Acc: 54.00709\n",
            "Train Loss: 0.00049 | Test Loss: 1.65651 | Test Acc: 52.92730\n",
            "Train Loss: 0.00038 | Test Loss: 1.65135 | Test Acc: 53.59309\n",
            "Train Loss: 0.00039 | Test Loss: 1.65797 | Test Acc: 53.36348\n",
            "Train Loss: 0.00038 | Test Loss: 1.64748 | Test Acc: 53.82447\n",
            "Train Loss: 0.00041 | Test Loss: 1.64171 | Test Acc: 54.38830\n",
            "Train Loss: 0.00037 | Test Loss: 1.65144 | Test Acc: 54.19858\n",
            "Train Loss: 0.00042 | Test Loss: 1.64377 | Test Acc: 54.24468\n",
            "Train Loss: 0.00061 | Test Loss: 1.64750 | Test Acc: 54.00355\n",
            "Train Loss: 0.00050 | Test Loss: 1.65217 | Test Acc: 53.65337\n",
            "Train Loss: 0.00057 | Test Loss: 1.67790 | Test Acc: 52.85284\n",
            "Train Loss: 0.00029 | Test Loss: 1.69824 | Test Acc: 52.03901\n",
            "Train Loss: 0.00043 | Test Loss: 1.70752 | Test Acc: 51.61436\n",
            "Train Loss: 0.00033 | Test Loss: 1.66830 | Test Acc: 53.44947\n",
            "Train Loss: 0.00042 | Test Loss: 1.66173 | Test Acc: 53.45833\n",
            "Train Loss: 0.00055 | Test Loss: 1.66660 | Test Acc: 53.10461\n",
            "Train Loss: 0.00055 | Test Loss: 1.66488 | Test Acc: 52.91135\n",
            "Train Loss: 0.00054 | Test Loss: 1.66337 | Test Acc: 53.31560\n",
            "Train Loss: 0.00056 | Test Loss: 1.64850 | Test Acc: 53.77926\n",
            "Train Loss: 0.00046 | Test Loss: 1.67551 | Test Acc: 52.98493\n",
            "Train Loss: 0.00048 | Test Loss: 1.67343 | Test Acc: 53.26773\n",
            "Train Loss: 0.00056 | Test Loss: 1.65703 | Test Acc: 53.42376\n",
            "Train Loss: 0.00047 | Test Loss: 1.65438 | Test Acc: 53.49113\n",
            "Train Loss: 0.00056 | Test Loss: 1.64644 | Test Acc: 53.73227\n",
            "Train Loss: 0.00038 | Test Loss: 1.64816 | Test Acc: 54.01241\n",
            "Train Loss: 0.00047 | Test Loss: 1.65501 | Test Acc: 54.07181\n",
            "Train Loss: 0.00046 | Test Loss: 1.64853 | Test Acc: 54.32535\n",
            "Train Loss: 0.00054 | Test Loss: 1.64212 | Test Acc: 54.40248\n",
            "Train Loss: 0.00057 | Test Loss: 1.62763 | Test Acc: 54.80142\n",
            "Train Loss: 0.00047 | Test Loss: 1.63036 | Test Acc: 54.32004\n",
            "Train Loss: 0.00044 | Test Loss: 1.63247 | Test Acc: 54.30053\n",
            "Train Loss: 0.00049 | Test Loss: 1.64263 | Test Acc: 54.09840\n",
            "Train Loss: 0.00040 | Test Loss: 1.63468 | Test Acc: 54.59486\n",
            "Train Loss: 0.00035 | Test Loss: 1.63424 | Test Acc: 54.41933\n",
            "Train Loss: 0.00052 | Test Loss: 1.62466 | Test Acc: 54.84309\n",
            "Train Loss: 0.00054 | Test Loss: 1.63147 | Test Acc: 54.58688\n",
            "Train Loss: 0.00051 | Test Loss: 1.61727 | Test Acc: 54.94681\n",
            "Train Loss: 0.00045 | Test Loss: 1.61895 | Test Acc: 54.98670\n",
            "Train Loss: 0.00040 | Test Loss: 1.63789 | Test Acc: 54.30496\n",
            "Train Loss: 0.00055 | Test Loss: 1.62716 | Test Acc: 54.70390\n",
            "Train Loss: 0.00030 | Test Loss: 1.61141 | Test Acc: 55.51330\n",
            "Train Loss: 0.00032 | Test Loss: 1.61367 | Test Acc: 55.44947\n",
            "Train Loss: 0.00045 | Test Loss: 1.63092 | Test Acc: 55.08777\n",
            "Train Loss: 0.00056 | Test Loss: 1.62736 | Test Acc: 54.84929\n",
            "Train Loss: 0.00056 | Test Loss: 1.62200 | Test Acc: 55.21897\n",
            "Train Loss: 0.00045 | Test Loss: 1.63144 | Test Acc: 54.91046\n",
            "Train Loss: 0.00034 | Test Loss: 1.62874 | Test Acc: 55.13564\n",
            "Train Loss: 0.00050 | Test Loss: 1.62233 | Test Acc: 55.25532\n",
            "Train Loss: 0.00055 | Test Loss: 1.63518 | Test Acc: 54.29965\n",
            "Train Loss: 0.00047 | Test Loss: 1.62155 | Test Acc: 55.07092\n",
            "Train Loss: 0.00049 | Test Loss: 1.62455 | Test Acc: 54.53901\n",
            "Train Loss: 0.00038 | Test Loss: 1.62304 | Test Acc: 54.79167\n",
            "Train Loss: 0.00040 | Test Loss: 1.62955 | Test Acc: 54.76507\n",
            "Train Loss: 0.00045 | Test Loss: 1.62143 | Test Acc: 55.36436\n",
            "Train Loss: 0.00040 | Test Loss: 1.61070 | Test Acc: 55.68972\n",
            "Train Loss: 0.00039 | Test Loss: 1.63304 | Test Acc: 54.94770\n",
            "Train Loss: 0.00044 | Test Loss: 1.62063 | Test Acc: 55.14096\n",
            "Train Loss: 0.00050 | Test Loss: 1.61828 | Test Acc: 55.16046\n",
            "Train Loss: 0.00041 | Test Loss: 1.62864 | Test Acc: 54.31649\n",
            "Train Loss: 0.00047 | Test Loss: 1.61240 | Test Acc: 55.61968\n",
            "Train Loss: 0.00032 | Test Loss: 1.61446 | Test Acc: 55.16401\n",
            "Train Loss: 0.00040 | Test Loss: 1.61333 | Test Acc: 55.19149\n",
            "Train Loss: 0.00047 | Test Loss: 1.61730 | Test Acc: 54.92110\n",
            "Train Loss: 0.00033 | Test Loss: 1.60767 | Test Acc: 55.44947\n",
            "Train Loss: 0.00041 | Test Loss: 1.61654 | Test Acc: 54.92376\n",
            "Train Loss: 0.00050 | Test Loss: 1.62174 | Test Acc: 54.73404\n",
            "Train Loss: 0.00048 | Test Loss: 1.61787 | Test Acc: 54.98316\n",
            "Train Loss: 0.00046 | Test Loss: 1.61532 | Test Acc: 55.31206\n",
            "Train Loss: 0.00048 | Test Loss: 1.61989 | Test Acc: 54.94060\n",
            "Train Loss: 0.00053 | Test Loss: 1.61979 | Test Acc: 54.49113\n",
            "Train Loss: 0.00046 | Test Loss: 1.63038 | Test Acc: 54.07535\n",
            "Train Loss: 0.00045 | Test Loss: 1.63015 | Test Acc: 54.11436\n",
            "Train Loss: 0.00045 | Test Loss: 1.63494 | Test Acc: 54.07004\n",
            "Train Loss: 0.00049 | Test Loss: 1.63091 | Test Acc: 54.47872\n",
            "Train Loss: 0.00059 | Test Loss: 1.62130 | Test Acc: 54.96631\n",
            "Train Loss: 0.00047 | Test Loss: 1.62930 | Test Acc: 54.57979\n",
            "Train Loss: 0.00045 | Test Loss: 1.63756 | Test Acc: 54.38209\n",
            "Train Loss: 0.00038 | Test Loss: 1.61390 | Test Acc: 55.19149\n",
            "Train Loss: 0.00037 | Test Loss: 1.61614 | Test Acc: 55.21277\n",
            "Train Loss: 0.00047 | Test Loss: 1.61689 | Test Acc: 55.18706\n",
            "Train Loss: 0.00048 | Test Loss: 1.62126 | Test Acc: 55.04610\n",
            "Train Loss: 0.00039 | Test Loss: 1.61243 | Test Acc: 55.37677\n",
            "Train Loss: 0.00033 | Test Loss: 1.62181 | Test Acc: 55.12855\n",
            "Train Loss: 0.00046 | Test Loss: 1.61634 | Test Acc: 55.43528\n",
            "Train Loss: 0.00047 | Test Loss: 1.63300 | Test Acc: 55.01596\n",
            "Train Loss: 0.00052 | Test Loss: 1.61853 | Test Acc: 55.09752\n",
            "Train Loss: 0.00027 | Test Loss: 1.63058 | Test Acc: 54.54699\n",
            "Train Loss: 0.00034 | Test Loss: 1.63221 | Test Acc: 54.23759\n",
            "Train Loss: 0.00046 | Test Loss: 1.62457 | Test Acc: 54.45922\n",
            "Train Loss: 0.00055 | Test Loss: 1.61235 | Test Acc: 54.83865\n",
            "Train Loss: 0.00045 | Test Loss: 1.61813 | Test Acc: 54.60195\n",
            "Train Loss: 0.00060 | Test Loss: 1.61112 | Test Acc: 54.85018\n",
            "Train Loss: 0.00049 | Test Loss: 1.61898 | Test Acc: 54.47784\n",
            "Train Loss: 0.00068 | Test Loss: 1.61303 | Test Acc: 54.78457\n",
            "Train Loss: 0.00054 | Test Loss: 1.61653 | Test Acc: 54.81560\n",
            "Train Loss: 0.00045 | Test Loss: 1.61222 | Test Acc: 55.07004\n",
            "Train Loss: 0.00043 | Test Loss: 1.62902 | Test Acc: 54.24823\n",
            "Train Loss: 0.00050 | Test Loss: 1.61460 | Test Acc: 54.71011\n",
            "Train Loss: 0.00046 | Test Loss: 1.60581 | Test Acc: 55.04078\n",
            "Train Loss: 0.00046 | Test Loss: 1.61786 | Test Acc: 54.72518\n",
            "Train Loss: 0.00058 | Test Loss: 1.61240 | Test Acc: 54.68617\n",
            "Train Loss: 0.00046 | Test Loss: 1.63130 | Test Acc: 54.13298\n",
            "Train Loss: 0.00044 | Test Loss: 1.60894 | Test Acc: 54.69858\n",
            "Train Loss: 0.00034 | Test Loss: 1.61027 | Test Acc: 54.61968\n",
            "Train Loss: 0.00034 | Test Loss: 1.60649 | Test Acc: 54.92021\n",
            "Train Loss: 0.00044 | Test Loss: 1.60650 | Test Acc: 54.69415\n",
            "Train Loss: 0.00060 | Test Loss: 1.61204 | Test Acc: 54.51862\n",
            "Train Loss: 0.00043 | Test Loss: 1.63096 | Test Acc: 53.84131\n",
            "Train Loss: 0.00042 | Test Loss: 1.60207 | Test Acc: 55.04344\n",
            "Train Loss: 0.00046 | Test Loss: 1.60276 | Test Acc: 55.16844\n",
            "Train Loss: 0.00039 | Test Loss: 1.60238 | Test Acc: 55.14007\n",
            "Train Loss: 0.00053 | Test Loss: 1.59334 | Test Acc: 55.51418\n",
            "Train Loss: 0.00059 | Test Loss: 1.60009 | Test Acc: 55.41844\n",
            "Train Loss: 0.00053 | Test Loss: 1.60016 | Test Acc: 55.69947\n",
            "Train Loss: 0.00043 | Test Loss: 1.59410 | Test Acc: 55.78635\n",
            "Train Loss: 0.00043 | Test Loss: 1.59686 | Test Acc: 55.37855\n",
            "Train Loss: 0.00052 | Test Loss: 1.60046 | Test Acc: 55.39362\n",
            "Train Loss: 0.00038 | Test Loss: 1.59552 | Test Acc: 55.78191\n",
            "Train Loss: 0.00045 | Test Loss: 1.60260 | Test Acc: 55.17287\n",
            "Train Loss: 0.00042 | Test Loss: 1.60328 | Test Acc: 55.39805\n",
            "Train Loss: 0.00039 | Test Loss: 1.59537 | Test Acc: 55.52305\n",
            "Train Loss: 0.00044 | Test Loss: 1.62665 | Test Acc: 54.08954\n",
            "Train Loss: 0.00065 | Test Loss: 1.60551 | Test Acc: 54.65426\n",
            "Train Loss: 0.00062 | Test Loss: 1.64073 | Test Acc: 53.49291\n",
            "Train Loss: 0.00051 | Test Loss: 1.61486 | Test Acc: 54.53369\n",
            "Train Loss: 0.00041 | Test Loss: 1.60633 | Test Acc: 54.68440\n",
            "Train Loss: 0.00049 | Test Loss: 1.61448 | Test Acc: 54.37766\n",
            "Train Loss: 0.00056 | Test Loss: 1.59172 | Test Acc: 55.21188\n",
            "Train Loss: 0.00046 | Test Loss: 1.59318 | Test Acc: 55.76507\n",
            "Train Loss: 0.00038 | Test Loss: 1.60114 | Test Acc: 55.28457\n",
            "Train Loss: 0.00046 | Test Loss: 1.57992 | Test Acc: 55.77216\n",
            "Train Loss: 0.00054 | Test Loss: 1.58257 | Test Acc: 55.77039\n",
            "Train Loss: 0.00060 | Test Loss: 1.58509 | Test Acc: 55.58865\n",
            "Train Loss: 0.00045 | Test Loss: 1.58221 | Test Acc: 55.98670\n",
            "Train Loss: 0.00045 | Test Loss: 1.61777 | Test Acc: 55.00887\n",
            "Train Loss: 0.00050 | Test Loss: 1.60858 | Test Acc: 55.15691\n",
            "Train Loss: 0.00039 | Test Loss: 1.60485 | Test Acc: 55.28191\n",
            "Train Loss: 0.00051 | Test Loss: 1.58918 | Test Acc: 55.62677\n",
            "Train Loss: 0.00055 | Test Loss: 1.58714 | Test Acc: 55.74645\n",
            "Train Loss: 0.00037 | Test Loss: 1.59842 | Test Acc: 55.57890\n",
            "Train Loss: 0.00047 | Test Loss: 1.58755 | Test Acc: 55.90426\n",
            "Train Loss: 0.00035 | Test Loss: 1.58059 | Test Acc: 55.81560\n",
            "Train Loss: 0.00052 | Test Loss: 1.59064 | Test Acc: 55.52394\n",
            "Train Loss: 0.00048 | Test Loss: 1.58885 | Test Acc: 55.40780\n",
            "Train Loss: 0.00060 | Test Loss: 1.58852 | Test Acc: 55.33865\n",
            "Train Loss: 0.00057 | Test Loss: 1.60393 | Test Acc: 54.84929\n",
            "Train Loss: 0.00048 | Test Loss: 1.60955 | Test Acc: 54.74379\n",
            "Train Loss: 0.00052 | Test Loss: 1.59418 | Test Acc: 55.37145\n",
            "Train Loss: 0.00050 | Test Loss: 1.57668 | Test Acc: 55.70922\n",
            "Train Loss: 0.00041 | Test Loss: 1.57659 | Test Acc: 55.49291\n",
            "Train Loss: 0.00045 | Test Loss: 1.57740 | Test Acc: 55.60904\n",
            "Train Loss: 0.00043 | Test Loss: 1.59285 | Test Acc: 55.16667\n",
            "Train Loss: 0.00046 | Test Loss: 1.60557 | Test Acc: 54.64539\n",
            "Train Loss: 0.00050 | Test Loss: 1.59735 | Test Acc: 54.83777\n",
            "Train Loss: 0.00026 | Test Loss: 1.59898 | Test Acc: 54.59220\n",
            "Train Loss: 0.00055 | Test Loss: 1.58307 | Test Acc: 55.20124\n",
            "Train Loss: 0.00034 | Test Loss: 1.59495 | Test Acc: 54.83245\n",
            "Train Loss: 0.00048 | Test Loss: 1.59410 | Test Acc: 55.14184\n",
            "Train Loss: 0.00046 | Test Loss: 1.59688 | Test Acc: 54.82713\n",
            "Train Loss: 0.00053 | Test Loss: 1.58380 | Test Acc: 55.12766\n",
            "Train Loss: 0.00044 | Test Loss: 1.59080 | Test Acc: 55.29965\n",
            "Train Loss: 0.00045 | Test Loss: 1.59617 | Test Acc: 54.84043\n",
            "Train Loss: 0.00058 | Test Loss: 1.59827 | Test Acc: 54.90426\n",
            "Train Loss: 0.00041 | Test Loss: 1.58746 | Test Acc: 55.47872\n",
            "Train Loss: 0.00037 | Test Loss: 1.57641 | Test Acc: 56.00798\n",
            "Train Loss: 0.00056 | Test Loss: 1.59348 | Test Acc: 55.12589\n",
            "Train Loss: 0.00047 | Test Loss: 1.58359 | Test Acc: 55.35018\n",
            "Train Loss: 0.00041 | Test Loss: 1.58061 | Test Acc: 55.36259\n",
            "Train Loss: 0.00032 | Test Loss: 1.58522 | Test Acc: 55.07535\n",
            "Train Loss: 0.00030 | Test Loss: 1.59234 | Test Acc: 54.95035\n",
            "Train Loss: 0.00068 | Test Loss: 1.61760 | Test Acc: 53.92110\n",
            "Train Loss: 0.00056 | Test Loss: 1.60785 | Test Acc: 54.32270\n",
            "Train Loss: 0.00059 | Test Loss: 1.61019 | Test Acc: 54.30142\n",
            "Train Loss: 0.00051 | Test Loss: 1.58794 | Test Acc: 55.37145\n",
            "Train Loss: 0.00059 | Test Loss: 1.58306 | Test Acc: 55.79167\n",
            "Train Loss: 0.00041 | Test Loss: 1.57778 | Test Acc: 55.65071\n",
            "Train Loss: 0.00045 | Test Loss: 1.57777 | Test Acc: 55.56915\n",
            "Train Loss: 0.00033 | Test Loss: 1.57917 | Test Acc: 55.53635\n",
            "Train Loss: 0.00035 | Test Loss: 1.59029 | Test Acc: 55.45567\n",
            "Train Loss: 0.00045 | Test Loss: 1.57606 | Test Acc: 55.92553\n",
            "Train Loss: 0.00050 | Test Loss: 1.58033 | Test Acc: 55.44149\n",
            "Train Loss: 0.00040 | Test Loss: 1.58022 | Test Acc: 55.35195\n",
            "Train Loss: 0.00053 | Test Loss: 1.58522 | Test Acc: 55.30142\n",
            "Train Loss: 0.00049 | Test Loss: 1.56765 | Test Acc: 55.89539\n",
            "Train Loss: 0.00043 | Test Loss: 1.57962 | Test Acc: 55.41578\n",
            "Train Loss: 0.00033 | Test Loss: 1.58360 | Test Acc: 55.34752\n",
            "Train Loss: 0.00026 | Test Loss: 1.57787 | Test Acc: 55.54167\n",
            "Train Loss: 0.00054 | Test Loss: 1.58497 | Test Acc: 55.19060\n",
            "Train Loss: 0.00036 | Test Loss: 1.59448 | Test Acc: 54.78901\n",
            "Train Loss: 0.00042 | Test Loss: 1.62864 | Test Acc: 53.38387\n",
            "Train Loss: 0.00044 | Test Loss: 1.59790 | Test Acc: 54.62234\n",
            "Train Loss: 0.00047 | Test Loss: 1.58332 | Test Acc: 55.48936\n",
            "Train Loss: 0.00041 | Test Loss: 1.57391 | Test Acc: 55.65869\n",
            "Train Loss: 0.00040 | Test Loss: 1.58635 | Test Acc: 55.30319\n",
            "Train Loss: 0.00047 | Test Loss: 1.56841 | Test Acc: 56.00532\n",
            "Train Loss: 0.00038 | Test Loss: 1.57376 | Test Acc: 55.86879\n",
            "Train Loss: 0.00040 | Test Loss: 1.59477 | Test Acc: 55.14982\n",
            "Train Loss: 0.00052 | Test Loss: 1.58172 | Test Acc: 55.51507\n",
            "Train Loss: 0.00040 | Test Loss: 1.58153 | Test Acc: 55.65957\n",
            "Train Loss: 0.00034 | Test Loss: 1.58485 | Test Acc: 55.41578\n",
            "Train Loss: 0.00037 | Test Loss: 1.58837 | Test Acc: 55.42465\n",
            "Train Loss: 0.00037 | Test Loss: 1.58732 | Test Acc: 55.17553\n",
            "Train Loss: 0.00048 | Test Loss: 1.58362 | Test Acc: 55.08156\n",
            "Train Loss: 0.00036 | Test Loss: 1.57872 | Test Acc: 55.03369\n",
            "Train Loss: 0.00040 | Test Loss: 1.58328 | Test Acc: 54.75266\n",
            "Train Loss: 0.00074 | Test Loss: 1.58840 | Test Acc: 55.04344\n",
            "Train Loss: 0.00052 | Test Loss: 1.57682 | Test Acc: 55.37323\n",
            "Train Loss: 0.00047 | Test Loss: 1.58765 | Test Acc: 54.97784\n",
            "Train Loss: 0.00033 | Test Loss: 1.58526 | Test Acc: 55.22163\n",
            "Train Loss: 0.00032 | Test Loss: 1.58874 | Test Acc: 55.22872\n",
            "Train Loss: 0.00044 | Test Loss: 1.59752 | Test Acc: 54.78812\n",
            "Train Loss: 0.00051 | Test Loss: 1.59110 | Test Acc: 55.27482\n",
            "Train Loss: 0.00051 | Test Loss: 1.60656 | Test Acc: 54.19060\n",
            "Train Loss: 0.00045 | Test Loss: 1.59227 | Test Acc: 55.34663\n",
            "Train Loss: 0.00050 | Test Loss: 1.58487 | Test Acc: 55.84929\n",
            "Train Loss: 0.00035 | Test Loss: 1.57400 | Test Acc: 55.83777\n",
            "Train Loss: 0.00057 | Test Loss: 1.57099 | Test Acc: 55.72518\n",
            "Train Loss: 0.00044 | Test Loss: 1.57739 | Test Acc: 55.48227\n",
            "Train Loss: 0.00044 | Test Loss: 1.57830 | Test Acc: 55.68883\n",
            "Train Loss: 0.00038 | Test Loss: 1.60738 | Test Acc: 54.72784\n",
            "Train Loss: 0.00039 | Test Loss: 1.59194 | Test Acc: 55.24823\n",
            "Train Loss: 0.00038 | Test Loss: 1.58154 | Test Acc: 55.87943\n",
            "Train Loss: 0.00043 | Test Loss: 1.55691 | Test Acc: 56.55408\n",
            "Train Loss: 0.00033 | Test Loss: 1.56621 | Test Acc: 56.15780\n",
            "Train Loss: 0.00057 | Test Loss: 1.56508 | Test Acc: 56.47961\n",
            "Train Loss: 0.00075 | Test Loss: 1.56052 | Test Acc: 56.46897\n",
            "Train Loss: 0.00045 | Test Loss: 1.57264 | Test Acc: 55.73848\n",
            "Train Loss: 0.00039 | Test Loss: 1.58298 | Test Acc: 55.44592\n",
            "Train Loss: 0.00037 | Test Loss: 1.57759 | Test Acc: 55.76152\n",
            "Train Loss: 0.00043 | Test Loss: 1.56790 | Test Acc: 55.92996\n",
            "Train Loss: 0.00036 | Test Loss: 1.56892 | Test Acc: 55.80319\n",
            "Train Loss: 0.00056 | Test Loss: 1.59219 | Test Acc: 55.02660\n",
            "Train Loss: 0.00046 | Test Loss: 1.59719 | Test Acc: 55.10993\n",
            "Train Loss: 0.00054 | Test Loss: 1.57638 | Test Acc: 55.81206\n",
            "Train Loss: 0.00046 | Test Loss: 1.57239 | Test Acc: 56.09840\n",
            "Train Loss: 0.00044 | Test Loss: 1.56874 | Test Acc: 56.39450\n",
            "Train Loss: 0.00040 | Test Loss: 1.57693 | Test Acc: 56.06383\n",
            "Train Loss: 0.00053 | Test Loss: 1.58645 | Test Acc: 55.51152\n",
            "Train Loss: 0.00066 | Test Loss: 1.58437 | Test Acc: 55.36525\n",
            "Train Loss: 0.00026 | Test Loss: 1.57126 | Test Acc: 55.99823\n",
            "Train Loss: 0.00039 | Test Loss: 1.56480 | Test Acc: 55.98493\n",
            "Train Loss: 0.00055 | Test Loss: 1.56014 | Test Acc: 56.38741\n",
            "Train Loss: 0.00043 | Test Loss: 1.57659 | Test Acc: 55.65426\n",
            "Train Loss: 0.00048 | Test Loss: 1.56951 | Test Acc: 55.86702\n",
            "Train Loss: 0.00040 | Test Loss: 1.56490 | Test Acc: 56.10638\n",
            "Train Loss: 0.00052 | Test Loss: 1.55945 | Test Acc: 56.23848\n",
            "Train Loss: 0.00050 | Test Loss: 1.56132 | Test Acc: 56.22695\n",
            "Train Loss: 0.00033 | Test Loss: 1.55682 | Test Acc: 56.31915\n",
            "Train Loss: 0.00041 | Test Loss: 1.55214 | Test Acc: 56.51596\n",
            "Train Loss: 0.00031 | Test Loss: 1.56146 | Test Acc: 56.07535\n",
            "Train Loss: 0.00033 | Test Loss: 1.56255 | Test Acc: 56.21365\n",
            "Train Loss: 0.00038 | Test Loss: 1.56252 | Test Acc: 56.34752\n",
            "Train Loss: 0.00044 | Test Loss: 1.57607 | Test Acc: 56.22606\n",
            "Train Loss: 0.00048 | Test Loss: 1.56339 | Test Acc: 56.39894\n",
            "Train Loss: 0.00052 | Test Loss: 1.56239 | Test Acc: 56.24025\n",
            "Train Loss: 0.00029 | Test Loss: 1.56208 | Test Acc: 56.35461\n",
            "Train Loss: 0.00038 | Test Loss: 1.57185 | Test Acc: 55.80319\n",
            "Train Loss: 0.00034 | Test Loss: 1.56141 | Test Acc: 56.23050\n",
            "Train Loss: 0.00057 | Test Loss: 1.56615 | Test Acc: 56.26507\n",
            "Train Loss: 0.00038 | Test Loss: 1.56989 | Test Acc: 55.65426\n",
            "Train Loss: 0.00050 | Test Loss: 1.58732 | Test Acc: 55.44060\n",
            "Train Loss: 0.00045 | Test Loss: 1.56696 | Test Acc: 56.17553\n",
            "Train Loss: 0.00055 | Test Loss: 1.56157 | Test Acc: 56.48404\n",
            "Train Loss: 0.00052 | Test Loss: 1.55222 | Test Acc: 56.40869\n",
            "Train Loss: 0.00048 | Test Loss: 1.56445 | Test Acc: 55.94504\n",
            "Train Loss: 0.00050 | Test Loss: 1.57121 | Test Acc: 55.76330\n",
            "Train Loss: 0.00047 | Test Loss: 1.56135 | Test Acc: 56.02571\n",
            "Train Loss: 0.00051 | Test Loss: 1.56325 | Test Acc: 55.94858\n",
            "Train Loss: 0.00062 | Test Loss: 1.57147 | Test Acc: 55.40869\n",
            "Train Loss: 0.00029 | Test Loss: 1.56061 | Test Acc: 55.53635\n",
            "Train Loss: 0.00054 | Test Loss: 1.57495 | Test Acc: 54.97429\n",
            "Train Loss: 0.00057 | Test Loss: 1.57018 | Test Acc: 55.05940\n",
            "Train Loss: 0.00055 | Test Loss: 1.56790 | Test Acc: 55.13741\n",
            "Train Loss: 0.00057 | Test Loss: 1.55840 | Test Acc: 55.99291\n",
            "Train Loss: 0.00037 | Test Loss: 1.56114 | Test Acc: 56.10816\n",
            "Train Loss: 0.00042 | Test Loss: 1.56682 | Test Acc: 55.85993\n",
            "Train Loss: 0.00038 | Test Loss: 1.56669 | Test Acc: 55.83688\n",
            "Train Loss: 0.00050 | Test Loss: 1.55114 | Test Acc: 56.67819\n",
            "Train Loss: 0.00036 | Test Loss: 1.55325 | Test Acc: 56.57181\n",
            "Train Loss: 0.00046 | Test Loss: 1.55934 | Test Acc: 56.45035\n",
            "Train Loss: 0.00049 | Test Loss: 1.56274 | Test Acc: 56.43174\n",
            "Train Loss: 0.00056 | Test Loss: 1.57051 | Test Acc: 56.05053\n",
            "Train Loss: 0.00077 | Test Loss: 1.55279 | Test Acc: 56.55319\n",
            "Train Loss: 0.00039 | Test Loss: 1.56481 | Test Acc: 55.88564\n",
            "Train Loss: 0.00047 | Test Loss: 1.61410 | Test Acc: 53.96365\n",
            "Train Loss: 0.00047 | Test Loss: 1.61980 | Test Acc: 53.88741\n",
            "Train Loss: 0.00051 | Test Loss: 1.59065 | Test Acc: 54.68351\n",
            "Train Loss: 0.00056 | Test Loss: 1.58025 | Test Acc: 55.16135\n",
            "Train Loss: 0.00054 | Test Loss: 1.57625 | Test Acc: 54.92642\n",
            "Train Loss: 0.00044 | Test Loss: 1.55207 | Test Acc: 56.08245\n",
            "Train Loss: 0.00040 | Test Loss: 1.56444 | Test Acc: 55.67465\n",
            "Train Loss: 0.00045 | Test Loss: 1.55380 | Test Acc: 56.11702\n",
            "Train Loss: 0.00077 | Test Loss: 1.56913 | Test Acc: 55.35018\n",
            "Train Loss: 0.00051 | Test Loss: 1.57689 | Test Acc: 55.21897\n",
            "Train Loss: 0.00041 | Test Loss: 1.57368 | Test Acc: 55.39805\n",
            "Train Loss: 0.00041 | Test Loss: 1.58473 | Test Acc: 54.57092\n",
            "Train Loss: 0.00037 | Test Loss: 1.55870 | Test Acc: 55.58865\n",
            "Train Loss: 0.00048 | Test Loss: 1.55944 | Test Acc: 55.47784\n",
            "Train Loss: 0.00061 | Test Loss: 1.55002 | Test Acc: 56.35018\n",
            "Train Loss: 0.00038 | Test Loss: 1.54277 | Test Acc: 56.37500\n",
            "Train Loss: 0.00036 | Test Loss: 1.55241 | Test Acc: 55.94858\n",
            "Train Loss: 0.00050 | Test Loss: 1.54353 | Test Acc: 56.35461\n",
            "Train Loss: 0.00036 | Test Loss: 1.53819 | Test Acc: 56.83954\n",
            "Train Loss: 0.00040 | Test Loss: 1.54160 | Test Acc: 56.62500\n",
            "Train Loss: 0.00047 | Test Loss: 1.54106 | Test Acc: 56.64362\n",
            "Train Loss: 0.00056 | Test Loss: 1.54341 | Test Acc: 56.50709\n",
            "Train Loss: 0.00049 | Test Loss: 1.55290 | Test Acc: 56.15869\n",
            "Train Loss: 0.00040 | Test Loss: 1.55396 | Test Acc: 56.41667\n",
            "Train Loss: 0.00053 | Test Loss: 1.55632 | Test Acc: 56.57181\n",
            "Train Loss: 0.00052 | Test Loss: 1.55666 | Test Acc: 56.39450\n",
            "Train Loss: 0.00044 | Test Loss: 1.57426 | Test Acc: 55.92730\n",
            "Train Loss: 0.00064 | Test Loss: 1.56108 | Test Acc: 56.22606\n",
            "Train Loss: 0.00043 | Test Loss: 1.55331 | Test Acc: 56.72872\n",
            "Train Loss: 0.00050 | Test Loss: 1.57929 | Test Acc: 55.81028\n",
            "Train Loss: 0.00035 | Test Loss: 1.56031 | Test Acc: 56.38918\n",
            "Train Loss: 0.00046 | Test Loss: 1.55797 | Test Acc: 56.39628\n",
            "Train Loss: 0.00054 | Test Loss: 1.60477 | Test Acc: 54.61968\n",
            "Train Loss: 0.00043 | Test Loss: 1.57360 | Test Acc: 55.66933\n",
            "Train Loss: 0.00039 | Test Loss: 1.58620 | Test Acc: 54.93440\n",
            "Train Loss: 0.00035 | Test Loss: 1.56598 | Test Acc: 55.58511\n",
            "Train Loss: 0.00055 | Test Loss: 1.55220 | Test Acc: 56.29610\n",
            "Train Loss: 0.00031 | Test Loss: 1.54925 | Test Acc: 56.16135\n",
            "Train Loss: 0.00045 | Test Loss: 1.55604 | Test Acc: 56.11436\n",
            "Looked at 25600/ 112800 samples\n",
            "Train Loss: 0.00032 | Test Loss: 1.55280 | Test Acc: 56.38652\n",
            "Train Loss: 0.00049 | Test Loss: 1.55866 | Test Acc: 55.93174\n",
            "Train Loss: 0.00047 | Test Loss: 1.54761 | Test Acc: 56.05496\n",
            "Train Loss: 0.00038 | Test Loss: 1.54098 | Test Acc: 56.62057\n",
            "Train Loss: 0.00050 | Test Loss: 1.55063 | Test Acc: 56.45567\n",
            "Train Loss: 0.00058 | Test Loss: 1.57295 | Test Acc: 55.72252\n",
            "Train Loss: 0.00038 | Test Loss: 1.56057 | Test Acc: 56.06472\n",
            "Train Loss: 0.00043 | Test Loss: 1.54741 | Test Acc: 56.57624\n",
            "Train Loss: 0.00033 | Test Loss: 1.55889 | Test Acc: 56.25532\n",
            "Train Loss: 0.00046 | Test Loss: 1.54534 | Test Acc: 56.55762\n",
            "Train Loss: 0.00041 | Test Loss: 1.55550 | Test Acc: 56.19592\n",
            "Train Loss: 0.00041 | Test Loss: 1.57038 | Test Acc: 55.28723\n",
            "Train Loss: 0.00046 | Test Loss: 1.54637 | Test Acc: 56.25709\n",
            "Train Loss: 0.00045 | Test Loss: 1.54784 | Test Acc: 56.25443\n",
            "Train Loss: 0.00038 | Test Loss: 1.54724 | Test Acc: 56.05762\n",
            "Train Loss: 0.00066 | Test Loss: 1.55686 | Test Acc: 55.73404\n",
            "Train Loss: 0.00041 | Test Loss: 1.57093 | Test Acc: 55.21543\n",
            "Train Loss: 0.00033 | Test Loss: 1.55760 | Test Acc: 55.57713\n",
            "Train Loss: 0.00039 | Test Loss: 1.55393 | Test Acc: 55.90780\n",
            "Train Loss: 0.00045 | Test Loss: 1.54841 | Test Acc: 56.13032\n",
            "Train Loss: 0.00050 | Test Loss: 1.53844 | Test Acc: 56.53723\n",
            "Train Loss: 0.00060 | Test Loss: 1.55630 | Test Acc: 56.21454\n",
            "Train Loss: 0.00033 | Test Loss: 1.55482 | Test Acc: 56.47163\n",
            "Train Loss: 0.00037 | Test Loss: 1.56258 | Test Acc: 56.21986\n",
            "Train Loss: 0.00044 | Test Loss: 1.53941 | Test Acc: 57.10993\n",
            "Train Loss: 0.00032 | Test Loss: 1.54262 | Test Acc: 57.29344\n",
            "Train Loss: 0.00035 | Test Loss: 1.54223 | Test Acc: 57.36082\n",
            "Train Loss: 0.00061 | Test Loss: 1.53414 | Test Acc: 57.51152\n",
            "Train Loss: 0.00040 | Test Loss: 1.52730 | Test Acc: 57.82801\n",
            "Train Loss: 0.00028 | Test Loss: 1.53250 | Test Acc: 57.70035\n",
            "Train Loss: 0.00048 | Test Loss: 1.52828 | Test Acc: 57.74557\n",
            "Train Loss: 0.00052 | Test Loss: 1.53344 | Test Acc: 57.46011\n",
            "Train Loss: 0.00052 | Test Loss: 1.56285 | Test Acc: 56.25975\n",
            "Train Loss: 0.00039 | Test Loss: 1.55602 | Test Acc: 56.43351\n",
            "Train Loss: 0.00051 | Test Loss: 1.56214 | Test Acc: 56.06649\n",
            "Train Loss: 0.00056 | Test Loss: 1.56468 | Test Acc: 55.66933\n",
            "Train Loss: 0.00036 | Test Loss: 1.56033 | Test Acc: 55.83688\n",
            "Train Loss: 0.00035 | Test Loss: 1.55649 | Test Acc: 56.13209\n",
            "Train Loss: 0.00047 | Test Loss: 1.54980 | Test Acc: 56.24911\n",
            "Train Loss: 0.00042 | Test Loss: 1.54083 | Test Acc: 56.74025\n",
            "Train Loss: 0.00041 | Test Loss: 1.54166 | Test Acc: 56.80496\n",
            "Train Loss: 0.00059 | Test Loss: 1.54279 | Test Acc: 56.52128\n",
            "Train Loss: 0.00049 | Test Loss: 1.54821 | Test Acc: 56.44326\n",
            "Train Loss: 0.00043 | Test Loss: 1.54652 | Test Acc: 56.68440\n",
            "Train Loss: 0.00055 | Test Loss: 1.54547 | Test Acc: 56.84309\n",
            "Train Loss: 0.00038 | Test Loss: 1.53506 | Test Acc: 57.02748\n",
            "Train Loss: 0.00066 | Test Loss: 1.54349 | Test Acc: 56.66844\n",
            "Train Loss: 0.00040 | Test Loss: 1.55071 | Test Acc: 56.39096\n",
            "Train Loss: 0.00035 | Test Loss: 1.56224 | Test Acc: 55.84043\n",
            "Train Loss: 0.00042 | Test Loss: 1.54236 | Test Acc: 56.63032\n",
            "Train Loss: 0.00039 | Test Loss: 1.54235 | Test Acc: 56.66578\n",
            "Train Loss: 0.00041 | Test Loss: 1.56039 | Test Acc: 55.95390\n",
            "Train Loss: 0.00051 | Test Loss: 1.53794 | Test Acc: 56.86613\n",
            "Train Loss: 0.00039 | Test Loss: 1.53313 | Test Acc: 57.17465\n",
            "Train Loss: 0.00045 | Test Loss: 1.54030 | Test Acc: 56.79876\n",
            "Train Loss: 0.00038 | Test Loss: 1.53028 | Test Acc: 57.09131\n",
            "Train Loss: 0.00048 | Test Loss: 1.52733 | Test Acc: 57.34574\n",
            "Train Loss: 0.00038 | Test Loss: 1.52942 | Test Acc: 57.16401\n",
            "Train Loss: 0.00059 | Test Loss: 1.55425 | Test Acc: 55.96809\n",
            "Train Loss: 0.00041 | Test Loss: 1.54765 | Test Acc: 56.08865\n",
            "Train Loss: 0.00056 | Test Loss: 1.53714 | Test Acc: 56.46277\n",
            "Train Loss: 0.00037 | Test Loss: 1.52602 | Test Acc: 56.99911\n",
            "Train Loss: 0.00033 | Test Loss: 1.52028 | Test Acc: 57.35461\n",
            "Train Loss: 0.00061 | Test Loss: 1.52097 | Test Acc: 57.46277\n",
            "Train Loss: 0.00041 | Test Loss: 1.51925 | Test Acc: 57.32270\n",
            "Train Loss: 0.00039 | Test Loss: 1.53168 | Test Acc: 56.60638\n",
            "Train Loss: 0.00036 | Test Loss: 1.53475 | Test Acc: 56.56649\n",
            "Train Loss: 0.00036 | Test Loss: 1.53894 | Test Acc: 56.21543\n",
            "Train Loss: 0.00042 | Test Loss: 1.54844 | Test Acc: 55.75177\n",
            "Train Loss: 0.00040 | Test Loss: 1.55266 | Test Acc: 55.61791\n",
            "Train Loss: 0.00037 | Test Loss: 1.54136 | Test Acc: 55.95035\n",
            "Train Loss: 0.00040 | Test Loss: 1.55048 | Test Acc: 55.93528\n",
            "Train Loss: 0.00041 | Test Loss: 1.54360 | Test Acc: 55.85461\n",
            "Train Loss: 0.00059 | Test Loss: 1.54608 | Test Acc: 56.08067\n",
            "Train Loss: 0.00057 | Test Loss: 1.53578 | Test Acc: 56.51330\n",
            "Train Loss: 0.00054 | Test Loss: 1.52986 | Test Acc: 56.54787\n"
          ]
        }
      ],
      "source": [
        "epochs = 15\n",
        "\n",
        "# training and testing loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"🌀 Epoch: {epoch} ----- \")\n",
        "\n",
        "  # training\n",
        "  train_loss = 0\n",
        "\n",
        "  # loop through training batches\n",
        "  for batch, (X, y) in enumerate(train_dataloader):\n",
        "    model_0.train()\n",
        "\n",
        "    # 1. Forward Pass\n",
        "    y_pred = model_0(X)\n",
        "\n",
        "    # 2. Caculate loss (per batch)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss  += loss # accumulatively add up to the loss per epoch\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss Backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # print out how many samples have been seen\n",
        "    if batch % 400 == 0:\n",
        "      print(f\"Looked at {batch * len(X)}/ {len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "    # average loss per batch per epoch\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    ## testing\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model_0.eval()\n",
        "    with torch.inference_mode():\n",
        "      for X, y in test_dataloader:\n",
        "        # 1. Forward pass\n",
        "        test_pred = model_0(X)\n",
        "\n",
        "        # 2. Calculate loss accumulation\n",
        "        test_loss += loss_fn(test_pred, y)\n",
        "\n",
        "        # 3. calculate accuracy preds need to be same as y_true\n",
        "        test_acc += accuracy_fn(y_true=y, y_pred = test_pred.argmax(dim=1))\n",
        "\n",
        "      # test loss\n",
        "      test_loss /= len(test_dataloader)\n",
        "\n",
        "      test_acc /= len(test_dataloader)\n",
        "\n",
        "      ### printing details\n",
        "      print(f\"Train Loss: {train_loss:.5f} | Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.5f}\")\n",
        "\n",
        "# Calculate the training time\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_train_model_0 = print_train_time(start=train_time_start_on_cpu,\n",
        "                                             end=train_time_end_on_cpu,\n",
        "                                             device=str(next(model_0.parameters()).device))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0VlywW2xaNr"
      },
      "source": [
        "## Make Predictions and get Model 0 results\n",
        "\n",
        "- function that takes trained model , a dataloader, loss function and accuracy function.\n",
        "- and we use function to make predictions and evaluate those predictions using the loss function and accuracy function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioegVRDjx7ph"
      },
      "outputs": [],
      "source": [
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn):\n",
        "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "        loss_fn (torch.nn.Module): The loss function of model.\n",
        "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "\n",
        "    Returns:\n",
        "        (dict): Results of model making predictions on data_loader.\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for X, y in data_loader:\n",
        "        # making predictions with the model\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # loss and acc. collections per batch\n",
        "        loss = loss + loss_fn(y_pred, y)\n",
        "        acc = acc + accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "      # Scale loss and acc to find the average loss/acc per batch\n",
        "      loss /= len(data_loader)\n",
        "      acc /= len(data_loader)\n",
        "\n",
        "      return {\n",
        "         \"model_name\": model.__classs__.__name__,\n",
        "         \"model_loss\": loss.item(),\n",
        "         \"model_acc\": acc   }\n",
        "\n",
        "# calculate the model result on the test dataset\n",
        "model_0_results = eval_model(model=model_0, data_loader=test_dataloader, loss_fn=loss_fn, accuracy_fn=accuracy_fn)\n",
        "model_0_results\n",
        "print(model_0_results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f2bb8f4529b435dafc4bcf4d840854c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ddf158236d54ff6b8f378714d770c8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "400d2dd41cfe4fc3bded697710eb7eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ddf158236d54ff6b8f378714d770c8c",
            "placeholder": "​",
            "style": "IPY_MODEL_a70aec9a889b41e595b633a22ac3e4ef",
            "value": "  0%"
          }
        },
        "4a89bb6a8a734e31a2ca25005a861516": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5edfbfa822e64a4ca2def088952d46f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7bf4bd4c2e8a443bb5d4a5057827a3b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83faee5101044db8a0bb5e55cc1e6fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a89bb6a8a734e31a2ca25005a861516",
            "placeholder": "​",
            "style": "IPY_MODEL_7bf4bd4c2e8a443bb5d4a5057827a3b0",
            "value": " 0/15 [00:00&lt;?, ?it/s]"
          }
        },
        "a40ec0c63a5040e894758ee22aef5137": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a70aec9a889b41e595b633a22ac3e4ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c41ba7fcec6246e2854fed1b99f8b996": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f2bb8f4529b435dafc4bcf4d840854c",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5edfbfa822e64a4ca2def088952d46f3",
            "value": 0
          }
        },
        "da98f92514b14a0986842d64e673857b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_400d2dd41cfe4fc3bded697710eb7eb9",
              "IPY_MODEL_c41ba7fcec6246e2854fed1b99f8b996",
              "IPY_MODEL_83faee5101044db8a0bb5e55cc1e6fe6"
            ],
            "layout": "IPY_MODEL_a40ec0c63a5040e894758ee22aef5137"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}